{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import datetime\n",
    "from glob import glob\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from general.basic import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Could not find this script file, setting __file__ = /mnt/coredata/processing/leads/code/working/scratchpad.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'proj': '/mnt/coredata/processing/leads',\n",
       " 'code': '/mnt/coredata/processing/leads/code',\n",
       " 'config': '/mnt/coredata/processing/leads/code/config',\n",
       " 'metadata': '/mnt/coredata/processing/leads/metadata',\n",
       " 'scans_to_process': '/mnt/coredata/processing/leads/metadata/scans_to_process',\n",
       " 'ssheets': '/mnt/coredata/processing/leads/metadata/ssheets',\n",
       " 'data': '/mnt/coredata/processing/leads/data',\n",
       " 'newdata': '/mnt/coredata/processing/leads/data/newdata',\n",
       " 'raw': '/mnt/coredata/processing/leads/data/raw',\n",
       " 'processed': '/mnt/coredata/processing/leads/data/processed'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set globals\n",
    "overwrite = True\n",
    "process_unused_mris = False\n",
    "\n",
    "# Find this file\n",
    "try:\n",
    "    print(f\"- Running {__file__}\")\n",
    "except NameError:\n",
    "    __file__ = \"/mnt/coredata/processing/leads/code/working/scratchpad.ipynb\"\n",
    "    print(f\"- Could not find this script file, setting __file__ = {__file__}\")\n",
    "\n",
    "# Set global paths and check that they exist\n",
    "ROOT = \"/mnt/coredata\"\n",
    "PATHS = {}\n",
    "if not __file__.startswith(ROOT):\n",
    "    raise ValueError(\n",
    "        f\"Could not parse the project path because {__file__} is not in {ROOT}\"\n",
    "    )\n",
    "PATHS[\"proj\"] = \"/\".join(__file__.split(\"/\")[:5])\n",
    "PATHS[\"code\"] = op.join(PATHS[\"proj\"], \"code\")\n",
    "PATHS[\"config\"] = op.join(PATHS[\"code\"], \"config\")\n",
    "PATHS[\"metadata\"] = op.join(PATHS[\"proj\"], \"metadata\")\n",
    "PATHS[\"scans_to_process\"] = op.join(PATHS[\"metadata\"], \"scans_to_process\")\n",
    "PATHS[\"ssheets\"] = op.join(PATHS[\"metadata\"], \"ssheets\")\n",
    "PATHS[\"data\"] = op.join(PATHS[\"proj\"], \"data\")\n",
    "PATHS[\"newdata\"] = op.join(PATHS[\"data\"], \"newdata\")\n",
    "PATHS[\"raw\"] = op.join(PATHS[\"data\"], \"raw\")\n",
    "PATHS[\"processed\"] = op.join(PATHS[\"data\"], \"processed\")\n",
    "for k, v in PATHS.items():\n",
    "    if not op.isdir(v):\n",
    "        raise ValueError(f\"Expected {v}, but this directory does not exist\")\n",
    "\n",
    "PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_raw(raw_dir, verbose=True):\n",
    "    \"\"\"Scrape raw directory for all nifti files and parse them.\n",
    "\n",
    "    Returns a pandas DataFrame with columns:\n",
    "    - subj: subject ID\n",
    "    - scan_date: scan date (YYYY-MM-DD)\n",
    "    - scan_type: scan type (MRI modality or PET tracer)\n",
    "    - raw_petf: full path to the nifti file in raw\n",
    "    \"\"\"\n",
    "    raw_scans = glob(op.join(raw_dir, \"**\", \"*.nii\"), recursive=True)\n",
    "\n",
    "    output = []\n",
    "    for scanf in raw_scans:\n",
    "        subj = _get_subj(scanf, raw_dir)\n",
    "        scan_date = _get_scan_date(scanf)\n",
    "        scan_type = _get_scan_type(scanf)\n",
    "        output.append([subj, scan_date, scan_type, scanf])\n",
    "\n",
    "    cols = [\"subj\", \"scan_date\", \"scan_type\", \"raw_petf\"]\n",
    "    output = pd.DataFrame(output, columns=cols)\n",
    "\n",
    "    # Add FDG to scan type for LONI files that don't save \"FDG\" in filename\n",
    "    output.loc[output[\"scan_type\"].isnull(), \"scan_type\"] = output.loc[\n",
    "        output[\"scan_type\"].isnull(), \"raw_petf\"\n",
    "    ].apply(lambda x: \"FDG\" if (op.join(raw_dir, \"fdg\") in x) else np.nan)\n",
    "\n",
    "    if verbose:\n",
    "        if len(output) == 0:\n",
    "            print(\"No scans found in raw\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Found {len(output)} scans from {output['subj'].nunique()} subjects in {raw_dir}\"\n",
    "            )\n",
    "    return output\n",
    "\n",
    "\n",
    "def _get_subj(filepath, raw_dir):\n",
    "    \"\"\"Return the subject ID from filepath to the recon'd nifti.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The filepath to the reconstructed nifti.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subj : str\n",
    "        The subject ID parsed from the input file basename.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subj = filepath.replace(raw_dir + \"/\", \"\").split(\"/\")[1]\n",
    "        if len(subj) > 0:\n",
    "            return subj\n",
    "        else:\n",
    "            return np.nan\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _get_scan_date(filepath):\n",
    "    \"\"\"Return the scan date from filepath to the recon'd nifti.\n",
    "\n",
    "    Iterates over filepath directories from right to left until it finds\n",
    "    a filename or directory whose first 10 characters matches the date\n",
    "    format YYYY-MM-DD.\n",
    "\n",
    "    Returns np.nan if no scan date is found, otherwise a string like\n",
    "    'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    for d in filepath.split(op.sep)[::-1]:\n",
    "        try:\n",
    "            acqdate = check_dt_fmt(d[:10], raise_error=True)\n",
    "            return acqdate\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def check_dt_fmt(datestr, raise_error=False):\n",
    "    \"\"\"Return datestr if formatted like YYYY-MM-DD.\n",
    "\n",
    "    If raise_error is True, raise a ValueError if datestr is not\n",
    "    formatted like YYYY-MM-DD. Otherwise return np.nan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datestr_to_datetime(datestr)\n",
    "        return datestr\n",
    "    except ValueError:\n",
    "        if raise_error:\n",
    "            raise ValueError(f\"{datestr} is not formatted like YYYY-MM-DD\")\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def datestr_to_datetime(datestr):\n",
    "    \"\"\"Convert a date string to a datetime object.\"\"\"\n",
    "    return datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def _get_scan_type(filepath, scan_types=None):\n",
    "    \"\"\"Parse the filepath and return the scan type.\"\"\"\n",
    "    if scan_types is None:\n",
    "        scan_types = load_scan_typesf(scan_typesf)\n",
    "    basename = op.basename(filepath).lower()\n",
    "    for k, v in scan_types.items():\n",
    "        if k in basename:\n",
    "            return v\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def date_diff(date1, date2, abs=False):\n",
    "    \"\"\"Return date2 - date1 in days.\"\"\"\n",
    "    try:\n",
    "        diff = (date2 - date1).days\n",
    "        if abs:\n",
    "            return np.abs(diff)\n",
    "        else:\n",
    "            return diff\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def find_closest_mri(\n",
    "    subj, scan_date, freesurfer_dir, limit_days=365, strict_limit=False\n",
    "):\n",
    "    \"\"\"Return closest MRI date, days from PET, and Freesurfer path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subj : str\n",
    "        The subject ID.\n",
    "    scan_date : str\n",
    "        Scan date (YYYY-MM-DD) to match the closest MRI scan to.\n",
    "    freesurfer_dir : str\n",
    "        Path to the top-level freesurfer directory containing individual\n",
    "        processed MRI directories like <subj>_<scan_date>.\n",
    "    limit_days : int\n",
    "        A warning is raised if no MRI scan is found within limit days\n",
    "        and np.nan is returned.\n",
    "    strict_limit : bool\n",
    "        If True, np.nan is returned if no MRI scan is found within limit days.\n",
    "    \"\"\"\n",
    "    proc_mris = glob(op.join(freesurfer_dir, f\"{subj}_*\"))\n",
    "    if len(proc_mris) == 0:\n",
    "        print(\n",
    "            f\"WARNING: {subj} scan on {scan_date} has no processed MRI scans in {freesurfer_dir}\"\n",
    "        )\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    proc_mri_dates = [check_dt_fmt(op.basename(p).split(\"_\")[1]) for p in proc_mris]\n",
    "\n",
    "    days_to_scan = []\n",
    "    for d in proc_mri_dates:\n",
    "        days_to_scan.append(\n",
    "            date_diff(datestr_to_datetime(d), datestr_to_datetime(scan_date), abs=True)\n",
    "        )\n",
    "    closest_mri = proc_mris[np.argmin(days_to_scan)]\n",
    "    closest_mri_date = proc_mri_dates[np.argmin(days_to_scan)]\n",
    "    min_days = min(days_to_scan)\n",
    "\n",
    "    if min_days > limit_days:\n",
    "        print(\n",
    "            f\"WARNING: {subj} scan on {scan_date} has no matching MRI within {limit_days} days\"\n",
    "        )\n",
    "        if strict_limit:\n",
    "            return np.nan, np.nan, np.nan\n",
    "\n",
    "    return closest_mri_date, min_days, closest_mri\n",
    "\n",
    "\n",
    "def get_upstream_path(start_path, target_dir_name=\"code\"):\n",
    "    \"\"\"Traverse upwards from start_path to find a directory with the name target_dir_name.\"\"\"\n",
    "    current_path = start_path\n",
    "    while op.dirname(current_path) != current_path:  # Check if we've reached the root\n",
    "        current_path = op.dirname(current_path)\n",
    "        if op.basename(current_path) == target_dir_name:\n",
    "            return current_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move newdata to raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Moving MRI and PET scan data from newdata to raw\n",
      " (newdata = /mnt/coredata/processing/leads/data/newdata)\n",
      " (raw     = /mnt/coredata/processing/leads/data/raw)\n",
      "  * Found 1241 directories with niftis\n"
     ]
    }
   ],
   "source": [
    "# Ensure the base directory paths are absolute and normalized\n",
    "newdata_dir = op.abspath(PATHS[\"newdata\"])\n",
    "raw_dir = op.abspath(PATHS[\"raw\"])\n",
    "do_cleanup = True\n",
    "overwrite = False\n",
    "verbose = True\n",
    "\n",
    "\n",
    "def do_cleanup():\n",
    "    print(\"Function removes everything in newdata here\")\n",
    "\n",
    "\n",
    "# Find all niftis in newdata\n",
    "check_exts = (\".nii\", \".nii.gz\", \".dcm\")\n",
    "glob_files = []\n",
    "for ext in check_exts:\n",
    "    glob_files.extend(glob(op.join(newdata_dir, f\"**/*{ext}\"), recursive=True))\n",
    "\n",
    "# Print the welcome message\n",
    "if verbose:\n",
    "    print(\"- Moving MRI and PET scan data from newdata to raw\")\n",
    "    print(f\" (newdata = {newdata_dir})\")\n",
    "    print(f\" (raw     = {raw_dir})\")\n",
    "\n",
    "# If no niftis are found, print a message and return\n",
    "if len(glob_files) == 0:\n",
    "    if verbose:\n",
    "        print(f\"  * No niftis found in {newdata_dir}\")\n",
    "    if cleanup:\n",
    "        do_cleanup()\n",
    "    if verbose:\n",
    "        print(\"\")\n",
    "    raise AssertionError(\"Function returns here\")\n",
    "\n",
    "# Find all unique nifti-containing directories in newdata\n",
    "source_dirs = set([op.dirname(f) for f in glob_files])\n",
    "if verbose:\n",
    "    print(f\"  * Found {len(source_dirs)} directories with niftis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_dirs = [\"LEADS\", \"ADNI\", \"IDEAS\", \"SCAN\", \"PAD\"]\n",
    "ii = 0\n",
    "for source_dir in source_dirs:\n",
    "    # Create a matching file hierarchy in raw as in newdata\n",
    "    scan_path = op.relpath(source_dir, newdata_dir)\n",
    "    scan_path_parts = scan_path.split(\"/\")\n",
    "    if scan_path_parts[0].upper() in skip_dirs:\n",
    "        scan_path = op.join(*scan_path_parts[1:])\n",
    "    target_dir = op.join(raw_dir, scan_path)\n",
    "    if ii > 9:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"source: {source_dir}\")\n",
    "        print(f\"target: {target_dir}\")\n",
    "        print(\"\")\n",
    "        ii += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_scans: (2067, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load the most recent raw_scans df\n",
    "raw_scans = pd.read_csv(\n",
    "    glob_sort_mtime(op.join(PATHS[\"metadata\"], \"log\", f\"raw_pet_scans_*.csv\"))[0]\n",
    ")\n",
    "raw_scans = raw_scans.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"raw_scans: {raw_scans.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match raw PET to closest MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = {}\n",
    "SCAN_TYPES = None\n",
    "TIMESTAMP = None\n",
    "\n",
    "\n",
    "def set_globals():\n",
    "    \"\"\"Set module-level global variables\"\"\"\n",
    "    global PATHS, SCAN_TYPES, TIMESTAMP\n",
    "\n",
    "    # The project directory is assumed to be the 4th directory up from /\n",
    "    # (e.g., /mnt/coredata/processing/leads)\n",
    "    PATHS[\"proj\"] = \"/mnt/coredata/processing/leads\"\n",
    "\n",
    "    # Set global paths and check that they exist\n",
    "    PATHS[\"code\"] = op.join(PATHS[\"proj\"], \"code\")\n",
    "    PATHS[\"config\"] = op.join(PATHS[\"code\"], \"config\")\n",
    "    PATHS[\"metadata\"] = op.join(PATHS[\"proj\"], \"metadata\")\n",
    "    PATHS[\"scans_to_process\"] = op.join(PATHS[\"metadata\"], \"scans_to_process\")\n",
    "    PATHS[\"ssheets\"] = op.join(PATHS[\"metadata\"], \"ssheets\")\n",
    "    PATHS[\"data\"] = op.join(PATHS[\"proj\"], \"data\")\n",
    "    PATHS[\"newdata\"] = op.join(PATHS[\"data\"], \"newdata\")\n",
    "    PATHS[\"raw\"] = op.join(PATHS[\"data\"], \"raw\")\n",
    "    PATHS[\"processed\"] = op.join(PATHS[\"data\"], \"processed\")\n",
    "    for k in PATHS:\n",
    "        if not op.isdir(PATHS[k]):\n",
    "            raise ValueError(f\"Expected {PATHS[k]}, but this directory does not exist\")\n",
    "\n",
    "    # Define the SCAN_TYPES dict\n",
    "    SCAN_TYPES = load_scan_typesf()\n",
    "\n",
    "    # Set the timestamp\n",
    "    TIMESTAMP = now()\n",
    "\n",
    "\n",
    "def load_scan_typesf(scan_typesf=None):\n",
    "    \"\"\"Load scan types CSV and return a {name_in: name_out} dict.\"\"\"\n",
    "    if scan_typesf is None:\n",
    "        scan_typesf = op.join(PATHS[\"config\"], \"scan_types_and_tracers.csv\")\n",
    "    scan_types = pd.read_csv(scan_typesf)\n",
    "    scan_types[\"name_in\"] = scan_types[\"name_in\"].str.lower()\n",
    "    scan_types = scan_types.drop_duplicates(\"name_in\").dropna()\n",
    "    scan_types = scan_types.set_index(\"name_in\")[\"name_out\"].to_dict()\n",
    "    return scan_types\n",
    "\n",
    "\n",
    "def now():\n",
    "    \"\"\"Return the current date and time down to seconds.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "\n",
    "set_globals()\n",
    "\n",
    "\n",
    "def main(overwrite, process_unused_mris):\n",
    "    \"\"\"Save CSV files for MRI and PET scans in the raw directory.\n",
    "\n",
    "    Indicate which scans need to be processed.\n",
    "    \"\"\"\n",
    "    # Print the welcome message\n",
    "    print(\"- Selecting scans to process\")\n",
    "\n",
    "    # Get a list of all directories containing .nii files\n",
    "    print(f\"  * Searching {PATHS['raw']} for all *.nii files\")\n",
    "    raw_niis = fast_recursive_glob_nii(PATHS[\"raw\"])\n",
    "\n",
    "    # Find the subject ID, scan type, acquisition date, and LONI image ID\n",
    "    # for each nifti file in raw\n",
    "    raw_niis = pd.DataFrame(raw_niis, columns=[\"raw_niif\"])\n",
    "    raw_niis.insert(0, \"subj\", raw_niis[\"raw_niif\"].apply(get_subj))\n",
    "    raw_niis.insert(1, \"scan_type\", raw_niis[\"raw_niif\"].apply(get_scan_type))\n",
    "    raw_niis.insert(2, \"scan_date\", raw_niis[\"raw_niif\"].apply(get_scan_date))\n",
    "    raw_niis.insert(3, \"image_id\", raw_niis[\"raw_niif\"].apply(get_image_id))\n",
    "\n",
    "    # Convert the date column to datetime\n",
    "    raw_niis[\"scan_date\"] = pd.to_datetime(raw_niis[\"scan_date\"])\n",
    "\n",
    "    # Separate MRI and PET scans into separate dataframes\n",
    "    raw_mris = raw_niis.query(\"(scan_type=='MRI-T1')\").reset_index(drop=True)\n",
    "    raw_pets = raw_niis.query(\"(scan_type!='MRI-T1')\").reset_index(drop=True)\n",
    "\n",
    "    # Rename columns\n",
    "    raw_mris = raw_mris.rename(\n",
    "        columns={\n",
    "            \"scan_date\": \"mri_date\",\n",
    "            \"image_id\": \"mri_image_id\",\n",
    "            \"raw_niif\": \"mri_raw_niif\",\n",
    "        }\n",
    "    )\n",
    "    raw_pets = raw_pets.rename(\n",
    "        columns={\n",
    "            \"scan_type\": \"tracer\",\n",
    "            \"scan_date\": \"pet_date\",\n",
    "            \"image_id\": \"pet_image_id\",\n",
    "            \"raw_niif\": \"pet_raw_niif\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get rid of scan_type column in the MRI dataframe (all scans are T1)\n",
    "    raw_mris = raw_mris.drop(columns=[\"scan_type\"])\n",
    "\n",
    "    # Sort MRIs by subject and scan date\n",
    "    raw_mris = raw_mris.sort_values([\"subj\", \"mri_date\"]).reset_index(drop=True)\n",
    "\n",
    "    # Sort PET scans by subject, tracer, and scan date\n",
    "    raw_pets = raw_pets.sort_values([\"subj\", \"tracer\", \"pet_date\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # Get PET resolution from filename\n",
    "    raw_pets.insert(\n",
    "        raw_pets.columns.tolist().index(\"pet_date\") + 1,\n",
    "        \"pet_res\",\n",
    "        raw_pets[\"pet_raw_niif\"].apply(get_pet_resolution),\n",
    "    )\n",
    "\n",
    "    # Report how many MRI and PET scans are in the raw directory\n",
    "    print(\n",
    "        \"  * Found {:,} niftis in {:,} subdirectories\".format(\n",
    "            len(raw_niis), len([op.dirname(f) for f in raw_niis[\"raw_niif\"]])\n",
    "        )\n",
    "    )\n",
    "    print(f\"    - {len(raw_mris):,} T1 MRIs\")\n",
    "    print(f\"    - {len(raw_pets):,} PET scans\")\n",
    "    for scan_type in raw_pets[\"tracer\"].unique():\n",
    "        print(\n",
    "            \"      * {} {}\".format(\n",
    "                len(raw_pets.loc[raw_pets[\"tracer\"] == scan_type]), scan_type\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Calculate the number of MRIs for each subject and the time between\n",
    "    # them\n",
    "    raw_mris = add_mri_date_columns(raw_mris)\n",
    "\n",
    "    # Calculate the number of PET scans for each subject and tracer, and\n",
    "    # the time between them\n",
    "    raw_pets = add_pet_date_columns(raw_pets)\n",
    "\n",
    "    # Match each PET scan to its closest MRI\n",
    "    raw_pets = find_closest_mri_to_pet(raw_pets, raw_mris)\n",
    "    print(\"  * Matching PET scans to their closest T1 MRIs\")\n",
    "\n",
    "    # Figure out which MRIs are actually used for PET processing\n",
    "    raw_mris[\"mri_used_for_pet_proc\"] = raw_mris[\"mri_image_id\"].apply(\n",
    "        lambda x: 1 if np.isin(x, raw_pets[\"mri_image_id\"]) else 0\n",
    "    )\n",
    "    idx = raw_mris.loc[raw_mris[\"mri_used_for_pet_proc\"] == 0].index.tolist()\n",
    "    if process_unused_mris:\n",
    "        msg = \"      to any PET scan but will be processed in any case\"\n",
    "    else:\n",
    "        msg = (\n",
    "            \"      to any PET scan and will not be added to the list of MRIs to process\"\n",
    "        )\n",
    "    print(\n",
    "        \"  * Auditing MRI scans\",\n",
    "        \"    - {:,}/{:,} MRIs in {} are not the closest MRI\".format(\n",
    "            len(idx),\n",
    "            len(raw_mris),\n",
    "            PATHS[\"raw\"],\n",
    "        ),\n",
    "        msg,\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "\n",
    "    # Flag PET scans with issues that preclude processing\n",
    "    raw_pets = audit_pet(raw_pets)\n",
    "    print(\"  * Auditing PET scans\")\n",
    "    print(\n",
    "        \"    - Flagged {:,} scans with issues to be resolved before processing\".format(\n",
    "            len(raw_pets.loc[raw_pets[\"flag\"] == 1])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add path to the processed MRI directory\n",
    "    ii = raw_mris.columns.tolist().index(\"mri_raw_niif\")\n",
    "    raw_mris[\"mri_proc_dir\"] = raw_mris.apply(\n",
    "        lambda x: get_mri_proc_dir(x[\"subj\"], x[\"mri_date\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Add path to the processed PET directory\n",
    "    ii = raw_pets.columns.tolist().index(\"pet_raw_niif\")\n",
    "    raw_pets.insert(\n",
    "        ii + 1,\n",
    "        \"pet_proc_dir\",\n",
    "        raw_pets.apply(\n",
    "            lambda x: get_pet_proc_dir(x[\"subj\"], x[\"tracer\"], x[\"pet_date\"]), axis=1\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Determine which MRIs have been processed\n",
    "    raw_mris[\"freesurfer_run\"] = raw_mris[\"mri_proc_dir\"].apply(check_if_freesurfer_run)\n",
    "    print(\n",
    "        \"  * {:,}/{:,} MRIs have been processed through Freesurfer\".format(\n",
    "            len(raw_mris.loc[raw_mris[\"freesurfer_run\"] == 1]),\n",
    "            len(raw_mris),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    raw_mris[\"mri_processed\"] = raw_mris[\"mri_proc_dir\"].apply(check_if_mri_processed)\n",
    "    print(\n",
    "        \"  * {:,}/{:,} MRIs have been fully processed\".format(\n",
    "            len(raw_mris.loc[raw_mris[\"mri_processed\"] == 1]),\n",
    "            len(raw_mris),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Determine which PET scans have been processed\n",
    "    raw_pets.insert(\n",
    "        ii + 1, \"pet_processed\", raw_pets[\"pet_proc_dir\"].apply(check_if_pet_processed)\n",
    "    )\n",
    "    print(\n",
    "        \"  * {:,}/{:,} PET scans have already been processed\".format(\n",
    "            len(raw_pets.loc[raw_pets[\"pet_processed\"] == 1]),\n",
    "            len(raw_pets),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Determine which MRI scans need to be processed\n",
    "    raw_mris[\"need_to_process\"] = raw_mris.apply(\n",
    "        lambda x: get_mri_to_process(\n",
    "            x[\"mri_used_for_pet_proc\"],\n",
    "            x[\"mri_processed\"],\n",
    "            overwrite,\n",
    "            process_unused_mris,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    print(\n",
    "        \"  * {:,}/{:,} MRIs are scheduled for processing\".format(\n",
    "            len(raw_mris.loc[raw_mris[\"need_to_process\"] == 1]),\n",
    "            len(raw_mris),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Determine which PET scans need to be processed\n",
    "    raw_pets[\"need_to_process\"] = raw_pets.apply(\n",
    "        lambda x: get_pet_to_process(x[\"flag\"], x[\"pet_processed\"], overwrite), axis=1\n",
    "    )\n",
    "    print(\n",
    "        \"  * {:,}/{:,} PET scans are scheduled for processing\".format(\n",
    "            len(raw_pets.loc[raw_pets[\"need_to_process\"] == 1]),\n",
    "            len(raw_pets),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert date columns to string\n",
    "    fmt = \"%Y-%m-%d\"\n",
    "    raw_mris[\"mri_date\"] = raw_mris[\"mri_date\"].dt.strftime(fmt)\n",
    "    for col in [\"pet_date\", \"mri_date\"]:\n",
    "        raw_pets[col] = raw_pets[col].dt.strftime(fmt)\n",
    "\n",
    "    # Save the raw MRI scans dataframe to a CSV file\n",
    "    save_mri_scan_index(raw_mris)\n",
    "\n",
    "    # Save the raw PET scans dataframe to a CSV file\n",
    "    save_pet_scan_index(raw_pets)\n",
    "\n",
    "\n",
    "def fast_recursive_glob_nii(path):\n",
    "    \"\"\"Return a list of all files in path that end in .nii\"\"\"\n",
    "\n",
    "    def _path_recurse(path):\n",
    "        \"\"\"Recursively traverse a directory and its subdirectories\"\"\"\n",
    "        with os.scandir(path) as entries:\n",
    "            for entry in entries:\n",
    "                if entry.is_dir():\n",
    "                    _path_recurse(entry.path)\n",
    "                elif entry.is_file() and entry.name.endswith(\".nii\"):\n",
    "                    nii_files.append(entry.path)\n",
    "\n",
    "    nii_files = []\n",
    "    _path_recurse(path)\n",
    "    return nii_files\n",
    "\n",
    "\n",
    "def glob_sort_mtime(pattern):\n",
    "    \"\"\"Return files matching pattern in most recent modified order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    files : list of str\n",
    "        List of files matching pattern, sorted by most recent modified\n",
    "        (files[0] is the most recently modified).\n",
    "    \"\"\"\n",
    "    files = sorted(glob(pattern), key=op.getmtime, reverse=True)\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_subj(filepath, raw_dir=None):\n",
    "    \"\"\"Return the subject ID from filepath to the recon'd nifti.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The filepath to the reconstructed nifti.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subj : str\n",
    "        The subject ID parsed from the input file basename.\n",
    "    \"\"\"\n",
    "    if raw_dir is None:\n",
    "        raw_dir = PATHS[\"raw\"]\n",
    "    try:\n",
    "        subj = filepath.replace(raw_dir + \"/\", \"\").split(\"/\")[0]\n",
    "        if len(subj) > 0:\n",
    "            return subj\n",
    "        else:\n",
    "            return np.nan\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_scan_type(filepath):\n",
    "    \"\"\"Search filepath and return the scan type\"\"\"\n",
    "\n",
    "    def find_matches(text, scan_types, scan_type_keys):\n",
    "        \"\"\"Helper function to find matches\"\"\"\n",
    "        matches = [key for key in scan_type_keys if key in text]\n",
    "        unique_values = list(set([scan_types[key] for key in matches]))\n",
    "        return unique_values\n",
    "\n",
    "    # Load the SCAN_TYPES dict and get lowercase keys\n",
    "    scan_type_keys = list(SCAN_TYPES)\n",
    "\n",
    "    # Convert filepath to lowercase and get the basename\n",
    "    filepath = filepath.lower()\n",
    "    basename = op.basename(filepath)\n",
    "\n",
    "    # First attempt to match in the basename\n",
    "    match_values = find_matches(basename, SCAN_TYPES, scan_type_keys)\n",
    "    if len(match_values) == 1:\n",
    "        return match_values[0]\n",
    "    elif len(match_values) > 1:\n",
    "        return \"FILENAME MATCHED MULTIPLE PET TRACERS OR MRI MODALITIES\"\n",
    "\n",
    "    # Check for specific substring in basename\n",
    "    if \"coreg,_avg,_std_img_and_vox_siz,_uniform\" in basename:\n",
    "        return \"FDG\"\n",
    "\n",
    "    # Attempt to match in the directory path after \"raw\"\n",
    "    dirname = op.dirname(filepath)\n",
    "    raw_base = \"/{}/\".format(op.basename(PATHS[\"raw\"]))\n",
    "    raw_idx = dirname.find(raw_base)\n",
    "    if raw_idx == -1:\n",
    "        return \"FAILED TO IDENTIFY PET TRACER OR MRI MODALITY FROM FILENAME\"\n",
    "\n",
    "    # Only consider part of the path after \"raw\"\n",
    "    search_path = dirname[raw_idx + len(raw_base) :]\n",
    "    match_values = find_matches(search_path, SCAN_TYPES, scan_type_keys)\n",
    "    if len(match_values) == 1:\n",
    "        return match_values[0]\n",
    "    elif len(match_values) > 1:\n",
    "        return \"FILENAME MATCHED MULTIPLE PET TRACERS OR MRI MODALITIES\"\n",
    "    else:\n",
    "        if \"coreg,_avg,_std_img_and_vox_siz,_uniform\" in search_path:\n",
    "            return \"FDG\"\n",
    "        else:\n",
    "            return \"FAILED TO IDENTIFY PET TRACER OR MRI MODALITY FROM FILENAME\"\n",
    "\n",
    "\n",
    "def get_scan_date(filepath):\n",
    "    \"\"\"Search filepath and return the scan date\"\"\"\n",
    "\n",
    "    def test_datestr(str_to_search, date_start=\"2000-01-01\", date_stop=None):\n",
    "        \"\"\"Return date if start of string is in YYYYMMDD format\n",
    "\n",
    "        The date must be >= date_start and <= today's date.\n",
    "        \"\"\"\n",
    "        # Remove hypens and underscores\n",
    "        str_to_search = str_to_search.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "        # Check if the first 8 characters are numeric\n",
    "        if str_to_search is None or not re.match(r\"^\\d{8}\", str_to_search):\n",
    "            return False\n",
    "\n",
    "        # Extract the date part of the string\n",
    "        datestr = \"{}-{}-{}\".format(\n",
    "            str_to_search[:4], str_to_search[4:6], str_to_search[6:8]\n",
    "        )\n",
    "\n",
    "        # Check if the date is valid and within the specified range\n",
    "        date_fmt = \"%Y-%m-%d\"\n",
    "        try:\n",
    "            date_test = datetime.datetime.strptime(datestr, date_fmt).date()\n",
    "            date_start = datetime.datetime.strptime(date_start, date_fmt).date()\n",
    "            if date_stop is None:\n",
    "                date_stop = datetime.date.today()\n",
    "            else:\n",
    "                date_stop = datetime.datetime.strptime(date_stop, date_fmt).date()\n",
    "            if date_start <= date_test <= date_stop:\n",
    "                return datestr\n",
    "            else:\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    basename = op.basename(filepath)\n",
    "    parts = basename.split(\"_\")\n",
    "    for part in parts:\n",
    "        datestr = test_datestr(part)\n",
    "        if datestr:\n",
    "            return datestr\n",
    "\n",
    "    # If no date was found in the basename, try the dirname but limit\n",
    "    # search to everything forward from the raw/ directory\n",
    "    dirname = op.dirname(filepath)\n",
    "    raw_base = \"/{}/\".format(op.basename(PATHS[\"raw\"]))\n",
    "    raw_idx = dirname.find(raw_base)\n",
    "    if raw_idx == -1:\n",
    "        return np.nan\n",
    "    search_path = dirname[raw_idx + len(raw_base) :]\n",
    "    parts = search_path.split(\"/\")\n",
    "    for part in parts:\n",
    "        datestr = test_datestr(part)\n",
    "        if datestr:\n",
    "            return datestr\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def datetime_to_datestr(dt):\n",
    "    \"\"\"Convert a datetime object to a date string.\"\"\"\n",
    "    try:\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_image_id(filepath):\n",
    "    \"\"\"Search basename and return the LONI Image ID\"\"\"\n",
    "\n",
    "    def test_image_id(str_to_search, pattern=r\"^I[0-9]+$\"):\n",
    "        \"\"\"Return str_to_search if it starts with 'I' and continues with numbers\"\"\"\n",
    "        if str_to_search is None:\n",
    "            return False\n",
    "        if bool(re.match(pattern, str_to_search)):\n",
    "            return str_to_search\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    basename = op.basename(filepath)\n",
    "    parts = basename.split(\"_\")\n",
    "    for part in parts:\n",
    "        image_id = test_image_id(part)\n",
    "        if image_id:\n",
    "            return image_id\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def get_pet_resolution(filepath):\n",
    "    \"\"\"Search basename and return the PET resolution\"\"\"\n",
    "\n",
    "    def test_resolution(str_to_search, pattern=r\"uniform_([0-9])mm_res\"):\n",
    "        \"\"\"Return scan resolution if the expected pattern is found\"\"\"\n",
    "        if str_to_search is None:\n",
    "            return False\n",
    "        match = re.search(pattern, str_to_search)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return False\n",
    "\n",
    "    res = test_resolution(op.basename(filepath).lower())\n",
    "    if res:\n",
    "        return res\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def add_mri_date_columns(mri_scans):\n",
    "    \"\"\"Add info on time between PET and MRI and adjacent PET scans\"\"\"\n",
    "    # Copy the input dataframe\n",
    "    mri_scans_cp = mri_scans.copy()\n",
    "\n",
    "    # Add columns for PET scan number and total number of scans per tracer\n",
    "    ii = mri_scans_cp.columns.tolist().index(\"mri_date\")\n",
    "    grp = mri_scans_cp.groupby(\"subj\")\n",
    "    mri_scans_cp.insert(ii + 1, \"mri_scan_number\", grp.cumcount() + 1)\n",
    "    mri_scans_cp.insert(\n",
    "        ii + 2, \"n_mri_scans\", grp[\"mri_scan_number\"].transform(\"count\")\n",
    "    )\n",
    "\n",
    "    # Add columns for days from each PET scan to baseline and days between\n",
    "    # consecutive PET scans per tracer\n",
    "    baseline_mri_dates = grp[\"mri_date\"].min()\n",
    "    mri_scans_cp.insert(\n",
    "        ii + 3,\n",
    "        \"days_from_baseline_mri\",\n",
    "        mri_scans_cp.apply(\n",
    "            lambda x: (x[\"mri_date\"] - baseline_mri_dates[x[\"subj\"]]).days,\n",
    "            axis=1,\n",
    "        ),\n",
    "    )\n",
    "    mri_scans_cp.insert(\n",
    "        ii + 4, \"days_from_last_mri\", grp[\"mri_date\"].diff().dt.days.fillna(0)\n",
    "    )\n",
    "\n",
    "    return mri_scans_cp\n",
    "\n",
    "\n",
    "def add_pet_date_columns(pet_scans):\n",
    "    \"\"\"Add info on time between PET and MRI and adjacent PET scans\"\"\"\n",
    "    # Copy the input dataframe\n",
    "    pet_scans_cp = pet_scans.copy()\n",
    "\n",
    "    # Add columns for PET scan number and total number of scans per tracer\n",
    "    ii = pet_scans_cp.columns.tolist().index(\"pet_date\")\n",
    "    grp = pet_scans_cp.groupby([\"subj\", \"tracer\"])\n",
    "    pet_scans_cp = pet_scans_cp.sort_values([\"subj\", \"tracer\", \"pet_date\"])\n",
    "    pet_scans_cp.insert(ii + 1, \"pet_scan_number\", grp.cumcount() + 1)\n",
    "    pet_scans_cp.insert(\n",
    "        ii + 2, \"n_pet_scans\", grp[\"pet_scan_number\"].transform(\"count\")\n",
    "    )\n",
    "\n",
    "    # Add columns for days from each PET scan to baseline and days between\n",
    "    # consecutive PET scans per tracer\n",
    "    baseline_pet_dates = grp[\"pet_date\"].min()\n",
    "    pet_scans_cp.insert(\n",
    "        ii + 3,\n",
    "        \"days_from_baseline_pet\",\n",
    "        pet_scans_cp.apply(\n",
    "            lambda x: (\n",
    "                x[\"pet_date\"] - baseline_pet_dates[(x[\"subj\"], x[\"tracer\"])]\n",
    "            ).days,\n",
    "            axis=1,\n",
    "        ),\n",
    "    )\n",
    "    pet_scans_cp.insert(\n",
    "        ii + 4, \"days_from_last_pet\", grp[\"pet_date\"].diff().dt.days.fillna(0)\n",
    "    )\n",
    "\n",
    "    return pet_scans_cp\n",
    "\n",
    "\n",
    "def find_closest_mri_to_pet(pet_scans, mri_scans):\n",
    "    \"\"\"Return a merged dataframe with the closest MRI to each PET scan\"\"\"\n",
    "    # Check that the necessary columns are present\n",
    "    assert np.all(\n",
    "        np.isin([\"subj\", \"tracer\", \"pet_date\", \"pet_image_id\"], pet_scans.columns)\n",
    "    )\n",
    "    assert np.all(\n",
    "        np.isin(\n",
    "            [\"subj\", \"mri_date\", \"mri_scan_number\", \"n_mri_scans\"], mri_scans.columns\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Copy the input dataframes\n",
    "    pet_scans_cp = pet_scans.copy()\n",
    "    mri_scans_cp = mri_scans.copy()\n",
    "\n",
    "    # Merge the dataframes\n",
    "    merged = pet_scans_cp.merge(mri_scans_cp, on=\"subj\", how=\"left\")\n",
    "\n",
    "    # Compute the date difference between each PET and MRI scan\n",
    "    ii = merged.columns.tolist().index(\"pet_date\")\n",
    "    merged.insert(\n",
    "        ii + 1,\n",
    "        \"pet_to_mri_days\",\n",
    "        (merged[\"pet_date\"] - merged[\"mri_date\"]).abs().dt.days,\n",
    "    )\n",
    "\n",
    "    # Sort by |pet_date - mri_date| and drop duplicates\n",
    "    merged_min = merged.sort_values(\"pet_to_mri_days\").drop_duplicates(\"pet_image_id\")\n",
    "\n",
    "    # Resort the dataframe and reset index before returning\n",
    "    merged_min = merged_min.sort_values([\"subj\", \"tracer\", \"pet_date\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return merged_min\n",
    "\n",
    "\n",
    "def audit_pet(pet_scans):\n",
    "    \"\"\"Audit each PET scan and flag scans with potential issues\"\"\"\n",
    "    pet_scans_cp = pet_scans.copy()\n",
    "    pet_scans_cp[\"flag\"] = 0\n",
    "    pet_scans_cp[\"notes\"] = \"\"\n",
    "\n",
    "    # Missing tracer\n",
    "    idx = pet_scans_cp.loc[\n",
    "        pet_scans_cp[\"tracer\"].str.startswith(\"FAILED TO IDENTIFY\")\n",
    "    ].index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[\n",
    "        idx, \"notes\"\n",
    "    ] += \"Unknown tracer, check raw PET filename against scan_types_and_tracers.csv\\n\"\n",
    "\n",
    "    # Multiple tracers\n",
    "    idx = pet_scans_cp.loc[\n",
    "        pet_scans_cp[\"tracer\"].str.startswith(\"FILENAME MATCHED MULTIPLE\")\n",
    "    ].index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[\n",
    "        idx, \"notes\"\n",
    "    ] += (\n",
    "        \"Matched >1 tracer, check raw PET filename against scan_types_and_tracers.csv\\n\"\n",
    "    )\n",
    "\n",
    "    # Missing PET date\n",
    "    idx = pet_scans_cp.loc[pd.isna(pet_scans_cp[\"pet_date\"])].index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[idx, \"notes\"] += \"Missing PET date\\n\"\n",
    "\n",
    "    # PET resolution unclear or not at 6mm\n",
    "    idx = pet_scans_cp.loc[pet_scans_cp[\"pet_res\"] != 6].index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[idx, \"notes\"] += \"PET resolution is unclear or not at 6mm\\n\"\n",
    "\n",
    "    # Missing MRI\n",
    "    idx = pet_scans_cp.loc[pd.isna(pet_scans_cp).any(axis=1)].index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[idx, \"notes\"] += f\"No available MRI in {PATHS['raw']}\\n\"\n",
    "\n",
    "    # PET and MRI scan dates > 365 days apart\n",
    "    idx = pet_scans_cp.query(\"(pet_to_mri_days>365)\").index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[idx, \"notes\"] += \"Closest MRI is more than 1 year from PET date\\n\"\n",
    "\n",
    "    # Same MRI used to process multiple PET scans for the same tracer\n",
    "    idx = pet_scans_cp.loc[\n",
    "        pet_scans_cp.groupby([\"subj\", \"tracer\"])[\"mri_image_id\"].transform(\n",
    "            lambda x: (\n",
    "                True if (np.max(np.unique(x, return_counts=True)[1]) > 1) else False\n",
    "            )\n",
    "        )\n",
    "    ].index.tolist()\n",
    "    pet_scans_cp.loc[idx, \"flag\"] = 1\n",
    "    pet_scans_cp.loc[\n",
    "        idx, \"notes\"\n",
    "    ] += \"Same MRI would be used to process multiple timepoints for the same PET tracer\\n\"\n",
    "\n",
    "    return pet_scans_cp\n",
    "\n",
    "\n",
    "def get_mri_proc_dir(subj, mri_date):\n",
    "    \"\"\"Return the processed MRI directory for each MRI scan\"\"\"\n",
    "    return op.join(\n",
    "        PATHS[\"processed\"], subj, \"MRI-T1_{}\".format(datetime_to_datestr(mri_date))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pet_proc_dir(subj, tracer, pet_date):\n",
    "    \"\"\"Return the processed PET directory for each PET scan\"\"\"\n",
    "    return op.join(\n",
    "        PATHS[\"processed\"],\n",
    "        subj,\n",
    "        \"{}_{}\".format(tracer, datetime_to_datestr(pet_date)),\n",
    "    )\n",
    "\n",
    "\n",
    "def check_if_freesurfer_run(mri_proc_dir):\n",
    "    \"\"\"Return True if the MRI scan has been processed by Freesurfer\"\"\"\n",
    "    freesurfer_dir = op.join(mri_proc_dir, \"freesurfer\")\n",
    "    if not op.isdir(freesurfer_dir):\n",
    "        return 0\n",
    "    # Search for the Freesurfer output directory\n",
    "    nuf = op.join(freesurfer_dir, \"mri\", \"nu.mgz\")\n",
    "    aparcf = op.join(freesurfer_dir, \"mri\", \"aparc+aseg.mgz\")\n",
    "    bstemf = op.join(freesurfer_dir, \"mri\", \"brainstemSsLabels.v12.FSvoxelSpace.mgz\")\n",
    "    if op.isfile(nuf) and op.isfile(aparcf) and op.isfile(bstemf):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def check_if_mri_processed(mri_proc_dir):\n",
    "    \"\"\"Return True if the PET scan has been processed\"\"\"\n",
    "    if not op.isdir(mri_proc_dir):\n",
    "        return 0\n",
    "    # Search for the warped and affine transformed nu.nii,\n",
    "    # respectively, as these are among the last files created in the MRI\n",
    "    # processing pipeline. Also make sure we find at least one mask.\n",
    "    mfiles = glob(op.join(mri_proc_dir, \"*mask-*.nii\"))\n",
    "    wfiles = glob(op.join(mri_proc_dir, \"w*_nu.nii\"))\n",
    "    afiles = glob(op.join(mri_proc_dir, \"a*_nu.nii\"))\n",
    "    if mfiles and wfiles and afiles:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def check_if_pet_processed(pet_proc_dir):\n",
    "    \"\"\"Return True if the PET scan has been processed\"\"\"\n",
    "    if not op.isdir(pet_proc_dir):\n",
    "        return 0\n",
    "    # Search for the warped and affine transformed PET SUVRs,\n",
    "    # respectively, as these are among the last files created in the PET\n",
    "    # processing pipeline\n",
    "    wfiles = glob(op.join(pet_proc_dir, \"w*suvr*.nii\"))\n",
    "    afiles = glob(op.join(pet_proc_dir, \"a*suvr*.nii\"))\n",
    "    if wfiles and afiles:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_mri_to_process(used_for_pet, processed, overwrite, process_unused_mris):\n",
    "    \"\"\"Return 1 if the MRI scan should be processed, otherwise 0\"\"\"\n",
    "    if process_unused_mris or used_for_pet:\n",
    "        if overwrite or not processed:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_pet_to_process(flagged, processed, overwrite):\n",
    "    \"\"\"Return 1 if the PET scan should be processed, otherwise 0\"\"\"\n",
    "    if not flagged:\n",
    "        if overwrite or not processed:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def save_mri_scan_index(mri_scans):\n",
    "    \"\"\"Save the MRI scan index to a CSV file\"\"\"\n",
    "    # Move any existing files to archive\n",
    "    archive_dir = op.join(PATHS[\"scans_to_process\"], \"archive\")\n",
    "    files = glob(op.join(PATHS[\"scans_to_process\"], \"Raw_MRI_Scan_Index_*.csv\"))\n",
    "    if files:\n",
    "        if not op.isdir(archive_dir):\n",
    "            os.makedirs(archive_dir)\n",
    "        for f in files:\n",
    "            os.rename(f, op.join(archive_dir, op.basename(f)))\n",
    "\n",
    "    # Save the mri_scans dataframe\n",
    "    outf = op.join(PATHS[\"scans_to_process\"], f\"Raw_MRI_Scan_Index_{TIMESTAMP}.csv\")\n",
    "    mri_scans.to_csv(outf, index=False)\n",
    "    print(f\"  * Saved raw MRI scan index to {outf}\")\n",
    "\n",
    "\n",
    "def save_pet_scan_index(pet_scans):\n",
    "    \"\"\"Save the PET scan index to a CSV file\"\"\"\n",
    "    # Move any existing files to archive\n",
    "    archive_dir = op.join(PATHS[\"scans_to_process\"], \"archive\")\n",
    "    files = glob(op.join(PATHS[\"scans_to_process\"], \"Raw_PET_Scan_Index_*.csv\"))\n",
    "    if files:\n",
    "        if not op.isdir(archive_dir):\n",
    "            os.makedirs(archive_dir)\n",
    "        for f in files:\n",
    "            os.rename(f, op.join(archive_dir, op.basename(f)))\n",
    "\n",
    "    # Save the pet_scans dataframe\n",
    "    outf = op.join(PATHS[\"scans_to_process\"], f\"Raw_PET_Scan_Index_{TIMESTAMP}.csv\")\n",
    "    pet_scans.to_csv(outf, index=False)\n",
    "    print(f\"  * Saved raw PET scan index to {outf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Searching /mnt/coredata/processing/leads/data/raw for all *.nii files\n",
      "  * Found 3,309 niftis in 3,309 subdirectories\n",
      "    - 1,240 T1 MRIs\n",
      "    - 2,069 PET scans\n",
      "      * 973 FBB\n",
      "      * 159 FDG\n",
      "      * 937 FTP\n"
     ]
    }
   ],
   "source": [
    "print(f\"  * Searching {PATHS['raw']} for all *.nii files\")\n",
    "raw_niis = fast_recursive_glob_nii(PATHS[\"raw\"])\n",
    "\n",
    "# Find the subject ID, scan type, acquisition date, and LONI image ID\n",
    "# for each nifti file in raw\n",
    "raw_niis = pd.DataFrame(raw_niis, columns=[\"raw_niif\"])\n",
    "raw_niis.insert(0, \"subj\", raw_niis[\"raw_niif\"].apply(get_subj))\n",
    "raw_niis.insert(1, \"scan_type\", raw_niis[\"raw_niif\"].apply(get_scan_type))\n",
    "raw_niis.insert(2, \"scan_date\", raw_niis[\"raw_niif\"].apply(get_scan_date))\n",
    "raw_niis.insert(3, \"image_id\", raw_niis[\"raw_niif\"].apply(get_image_id))\n",
    "\n",
    "# Convert the date column to datetime\n",
    "raw_niis[\"scan_date\"] = pd.to_datetime(raw_niis[\"scan_date\"])\n",
    "\n",
    "# Separate MRI and PET scans into separate dataframes\n",
    "raw_mris = raw_niis.query(\"(scan_type=='MRI-T1')\").reset_index(drop=True)\n",
    "raw_pets = raw_niis.query(\"(scan_type!='MRI-T1')\").reset_index(drop=True)\n",
    "\n",
    "# Rename columns\n",
    "raw_mris = raw_mris.rename(\n",
    "    columns={\n",
    "        \"scan_date\": \"mri_date\",\n",
    "        \"image_id\": \"mri_image_id\",\n",
    "        \"raw_niif\": \"mri_raw_niif\",\n",
    "    }\n",
    ")\n",
    "raw_pets = raw_pets.rename(\n",
    "    columns={\n",
    "        \"scan_type\": \"tracer\",\n",
    "        \"scan_date\": \"pet_date\",\n",
    "        \"image_id\": \"pet_image_id\",\n",
    "        \"raw_niif\": \"pet_raw_niif\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get rid of scan_type column in the MRI dataframe (all scans are T1)\n",
    "raw_mris = raw_mris.drop(columns=[\"scan_type\"])\n",
    "\n",
    "# Sort MRIs by subject and scan date\n",
    "raw_mris = raw_mris.sort_values([\"subj\", \"mri_date\"]).reset_index(drop=True)\n",
    "\n",
    "# Sort PET scans by subject, tracer, and scan date\n",
    "raw_pets = raw_pets.sort_values([\"subj\", \"tracer\", \"pet_date\"]).reset_index(drop=True)\n",
    "\n",
    "# Get PET resolution from filename\n",
    "raw_pets.insert(\n",
    "    raw_pets.columns.tolist().index(\"pet_date\") + 1,\n",
    "    \"pet_res\",\n",
    "    raw_pets[\"pet_raw_niif\"].apply(get_pet_resolution),\n",
    ")\n",
    "\n",
    "# Report how many MRI and PET scans are in the raw directory\n",
    "print(\n",
    "    \"  * Found {:,} niftis in {:,} subdirectories\".format(\n",
    "        len(raw_niis), len([op.dirname(f) for f in raw_niis[\"raw_niif\"]])\n",
    "    )\n",
    ")\n",
    "print(f\"    - {len(raw_mris):,} T1 MRIs\")\n",
    "print(f\"    - {len(raw_pets):,} PET scans\")\n",
    "for scan_type in raw_pets[\"tracer\"].unique():\n",
    "    print(\n",
    "        \"      * {} {}\".format(\n",
    "            len(raw_pets.loc[raw_pets[\"tracer\"] == scan_type]), scan_type\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>tracer</th>\n",
       "      <th>pet_date</th>\n",
       "      <th>pet_scan_number</th>\n",
       "      <th>n_pet_scans</th>\n",
       "      <th>pet_res</th>\n",
       "      <th>pet_image_id</th>\n",
       "      <th>pet_raw_niif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779724</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>FDG</td>\n",
       "      <td>2021-07-14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779574</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778313</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779005</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778781</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1777871</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779415</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2019-08-21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1777826</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1777397</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779041</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778267</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LDS0070174</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2019-09-10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778181</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LDS0070174</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778510</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LDS0070174</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2021-10-12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779474</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LDS0070174</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779336</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LDS0070174</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I1779652</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LDS0070174</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2021-10-13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778002</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LDS0070176</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I1777607</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LDS0070176</td>\n",
       "      <td>FDG</td>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778468</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LDS0070176</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I1778674</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subj tracer   pet_date  pet_scan_number  n_pet_scans  pet_res  \\\n",
       "0   LDS0070120    FBB 2019-06-19                1            1        6   \n",
       "1   LDS0070120    FDG 2021-07-14                1            1        6   \n",
       "2   LDS0070120    FTP 2019-06-20                1            1        6   \n",
       "3   LDS0070166    FBB 2019-08-22                1            4        6   \n",
       "4   LDS0070166    FBB 2020-09-16                2            4        6   \n",
       "5   LDS0070166    FBB 2021-10-28                3            4        6   \n",
       "6   LDS0070166    FBB 2022-11-08                4            4        6   \n",
       "7   LDS0070166    FTP 2019-08-21                1            4        6   \n",
       "8   LDS0070166    FTP 2020-09-11                2            4        6   \n",
       "9   LDS0070166    FTP 2021-10-29                3            4        6   \n",
       "10  LDS0070166    FTP 2022-11-04                4            4        6   \n",
       "11  LDS0070174    FBB 2019-09-10                1            3        6   \n",
       "12  LDS0070174    FBB 2020-10-07                2            3        6   \n",
       "13  LDS0070174    FBB 2021-10-12                3            3        6   \n",
       "14  LDS0070174    FTP 2019-09-11                1            3        6   \n",
       "15  LDS0070174    FTP 2020-10-08                2            3        6   \n",
       "16  LDS0070174    FTP 2021-10-13                3            3        6   \n",
       "17  LDS0070176    FBB 2019-09-12                1            1        6   \n",
       "18  LDS0070176    FDG 2021-10-29                1            1        6   \n",
       "19  LDS0070176    FTP 2019-09-13                1            1        6   \n",
       "\n",
       "   pet_image_id                                       pet_raw_niif  \n",
       "0      I1779724  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "1      I1779574  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "2      I1778313  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "3      I1779005  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "4      I1778781  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "5      I1777871  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "6      I1779415  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "7      I1777826  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "8      I1777397  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "9      I1779041  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "10     I1778267  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "11     I1778181  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "12     I1778510  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "13     I1779474  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "14     I1779336  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "15     I1779652  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "16     I1778002  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "17     I1777607  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "18     I1778468  /mnt/coredata/processing/leads/data/raw/LDS007...  \n",
       "19     I1778674  /mnt/coredata/processing/leads/data/raw/LDS007...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pet_scans_cp.insert(ii + 1, \"pet_scan_number\", grp.cumcount() + 1)\n",
    "pet_scans_cp.insert(ii + 2, \"n_pet_scans\", grp[\"pet_scan_number\"].transform(\"count\"))\n",
    "pet_scans_cp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the input dataframe\n",
    "pet_scans_cp = raw_pets.copy()\n",
    "\n",
    "# Add columns for PET scan number and total number of scans per tracer\n",
    "ii = pet_scans_cp.columns.tolist().index(\"pet_date\")\n",
    "grp = pet_scans_cp.groupby([\"subj\", \"tracer\"])\n",
    "# pet_scans_cp = pet_scans_cp.sort_values([\"subj\", \"tracer\", \"pet_date\"])\n",
    "pet_scans_cp.insert(ii + 1, \"pet_scan_number\", grp.cumcount() + 1)\n",
    "pet_scans_cp.insert(ii + 2, \"n_pet_scans\", grp[\"pet_scan_number\"].transform(\"count\"))\n",
    "\n",
    "# Add columns for days from each PET scan to baseline and days between\n",
    "# consecutive PET scans per tracer\n",
    "baseline_pet_dates = grp[\"pet_date\"].min()\n",
    "pet_scans_cp.insert(\n",
    "    ii + 3,\n",
    "    \"days_from_baseline_pet\",\n",
    "    pet_scans_cp.apply(\n",
    "        lambda x: (x[\"pet_date\"] - baseline_pet_dates[(x[\"subj\"], x[\"tracer\"])]).days,\n",
    "        axis=1,\n",
    "    ),\n",
    ")\n",
    "pet_scans_cp.insert(\n",
    "    ii + 4, \"days_from_last_pet\", grp[\"pet_date\"].diff().dt.days.fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pet_date                  2024-04-02 00:00:00\n",
       "pet_scan_number                             4\n",
       "n_pet_scans                                 4\n",
       "days_from_baseline_pet                   1686\n",
       "days_from_last_pet                     1490.0\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pet_scans_cp.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"pet_date\",\n",
    "        \"pet_scan_number\",\n",
    "        \"n_pet_scans\",\n",
    "        \"days_from_baseline_pet\",\n",
    "        \"days_from_last_pet\",\n",
    "    ],\n",
    "].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Searching /mnt/coredata/processing/leads/data/raw for all *.nii files\n",
      "  * Found 3,306 .nii files in 3,306 subdirectories\n",
      "    - 1,240 T1 MRIs\n",
      "    - 2,066 PET scans\n",
      "      * 973 FBB\n",
      "      * 937 FTP\n",
      "      * 156 FDG\n",
      "  * Matching PET scans to their closest T1 MRIs\n",
      "  * Auditing MRI scans\n",
      "    - 145/1,240 MRIs in /mnt/coredata/processing/leads/data/raw are not the closest MRI\n",
      "      to any PET scan and will not be added to the list of MRIs to process\n",
      "  * 1,088/1,240 MRIs have been processed through Freesurfer\n",
      "  * 0/1,240 MRIs have been fully processed\n",
      "  * 1,095/1,240 MRIs are scheduled for processing\n",
      "  * Saved raw MRI scan index to /mnt/coredata/processing/leads/metadata/scans_to_process/Raw_MRI_Scan_Index_2024-05-14-17-09-16.csv\n",
      "  * Calculating days between scans\n",
      "  * Auditing PET scans\n",
      "    - Flagged 28 scans with issues that may need to be resolved before processing\n",
      "  * 0/2,066 PET scans have already been processed\n",
      "  * 2,038/2,066 PET scans are scheduled for processing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'merged_min' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 186\u001b[0m\n\u001b[1;32m    184\u001b[0m fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpet_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmri_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 186\u001b[0m     merged_min[col] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_min\u001b[49m[col]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(fmt)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Save the raw PET scans dataframe to a CSV file\u001b[39;00m\n\u001b[1;32m    189\u001b[0m outf \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mjoin(PATHS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscans_to_process\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw_PET_Scan_Index_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_min' is not defined"
     ]
    }
   ],
   "source": [
    "# Get a list of all directories containing .nii files\n",
    "print(f\"- Searching {PATHS['raw']} for all *.nii files\")\n",
    "raw_niis = fast_recursive_glob_nii(PATHS[\"raw\"])\n",
    "\n",
    "# Load the scan types CSV file\n",
    "# __file__ = \"/mnt/coredata/processing/leads/code/working/scratchpad.ipynb\"\n",
    "scan_types = load_scan_typesf()\n",
    "scan_type_keys = [x.lower() for x in list(scan_types)]\n",
    "\n",
    "# Find the subject ID, scan type, acquisition date, and LONI image ID\n",
    "# for each nifti file in raw\n",
    "raw_niis = pd.DataFrame(raw_niis, columns=[\"raw_niif\"])\n",
    "raw_niis.insert(0, \"subj\", raw_niis[\"raw_niif\"].apply(get_subj))\n",
    "raw_niis.insert(1, \"scan_type\", raw_niis[\"raw_niif\"].apply(get_scan_type))\n",
    "raw_niis.insert(2, \"scan_date\", raw_niis[\"raw_niif\"].apply(get_scan_date))\n",
    "raw_niis.insert(3, \"image_id\", raw_niis[\"raw_niif\"].apply(get_image_id))\n",
    "\n",
    "# Separate MRI and PET scans into separate dataframes\n",
    "raw_mris = raw_niis.query(\"(scan_type=='MRI-T1')\").reset_index(drop=True)\n",
    "raw_pets = raw_niis.query(\"(scan_type!='MRI-T1')\").reset_index(drop=True)\n",
    "\n",
    "# Rename columns\n",
    "raw_mris = raw_mris.rename(\n",
    "    columns={\n",
    "        \"scan_date\": \"mri_date\",\n",
    "        \"image_id\": \"mri_image_id\",\n",
    "        \"raw_niif\": \"mri_raw_niif\",\n",
    "    }\n",
    ")\n",
    "raw_pets = raw_pets.rename(\n",
    "    columns={\n",
    "        \"scan_type\": \"tracer\",\n",
    "        \"scan_date\": \"pet_date\",\n",
    "        \"image_id\": \"pet_image_id\",\n",
    "        \"raw_niif\": \"pet_raw_niif\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get rid of scan_type column in the MRI dataframe (all scans are T1)\n",
    "raw_mris = raw_mris.drop(columns=[\"scan_type\"])\n",
    "\n",
    "# Sort MRIs by subject and scan date\n",
    "raw_mris = raw_mris.sort_values([\"subj\", \"mri_date\"]).reset_index(drop=True)\n",
    "\n",
    "# Get PET resolution from filename\n",
    "raw_pets.insert(\n",
    "    raw_pets.columns.tolist().index(\"pet_date\") + 1,\n",
    "    \"pet_res\",\n",
    "    raw_pets[\"pet_raw_niif\"].apply(get_pet_resolution),\n",
    ")\n",
    "\n",
    "# Print some basic stats on many scans we've found in the raw directory\n",
    "print(\n",
    "    \"  * Found {:,} .nii files in {:,} subdirectories\".format(\n",
    "        len(raw_niis), len([op.dirname(f) for f in raw_niis[\"raw_niif\"]])\n",
    "    )\n",
    ")\n",
    "print(f\"    - {len(raw_mris):,} T1 MRIs\")\n",
    "print(f\"    - {len(raw_pets):,} PET scans\")\n",
    "for scan_type in raw_pets[\"tracer\"].unique():\n",
    "    print(\n",
    "        \"      * {} {}\".format(\n",
    "            len(raw_pets.loc[raw_pets[\"tracer\"] == scan_type]), scan_type\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Match each PET scan to its closest MRI\n",
    "raw_pets = find_closest_mri_to_pet(raw_pets, raw_mris)\n",
    "print(\"  * Matching PET scans to their closest T1 MRIs\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Add MRI date columns\n",
    "raw_mris = add_mri_date_columns(raw_mris)\n",
    "\n",
    "# Check if each MRI is used for PET processing\n",
    "raw_mris[\"mri_used_for_pet_proc\"] = raw_mris[\"mri_image_id\"].apply(\n",
    "    lambda x: 1 if np.isin(x, raw_pets[\"mri_image_id\"]) else 0\n",
    ")\n",
    "idx = raw_mris.loc[raw_mris[\"mri_used_for_pet_proc\"] == 0].index.tolist()\n",
    "if process_unused_mris:\n",
    "    msg = \"      to any PET scan but will be processed in any case\"\n",
    "else:\n",
    "    msg = \"      to any PET scan and will not be added to the list of MRIs to process\"\n",
    "print(\n",
    "    \"  * Auditing MRI scans\",\n",
    "    \"    - {:,}/{:,} MRIs in {} are not the closest MRI\".format(\n",
    "        len(idx),\n",
    "        len(raw_mris),\n",
    "        PATHS[\"raw\"],\n",
    "    ),\n",
    "    msg,\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "# Add path to the processed MRI directory\n",
    "ii = raw_mris.columns.tolist().index(\"mri_raw_niif\")\n",
    "raw_mris[\"mri_proc_dir\"] = raw_mris.apply(\n",
    "    lambda x: get_mri_proc_dir(x[\"subj\"], x[\"mri_date\"]), axis=1\n",
    ")\n",
    "\n",
    "# Determine which MRIs have been processed\n",
    "raw_mris[\"freesurfer_run\"] = raw_mris[\"mri_proc_dir\"].apply(check_if_freesurfer_run)\n",
    "print(\n",
    "    \"  * {:,}/{:,} MRIs have been processed through Freesurfer\".format(\n",
    "        len(raw_mris.loc[raw_mris[\"freesurfer_run\"] == 1]),\n",
    "        len(raw_mris),\n",
    "    )\n",
    ")\n",
    "\n",
    "raw_mris[\"mri_processed\"] = raw_mris[\"mri_proc_dir\"].apply(check_if_mri_processed)\n",
    "print(\n",
    "    \"  * {:,}/{:,} MRIs have been fully processed\".format(\n",
    "        len(raw_mris.loc[raw_mris[\"mri_processed\"] == 1]),\n",
    "        len(raw_mris),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Determine which MRI scans need to be processed\n",
    "raw_mris[\"need_to_process\"] = raw_mris.apply(\n",
    "    lambda x: get_mri_to_process(\n",
    "        x[\"mri_used_for_pet_proc\"], x[\"mri_processed\"], overwrite, process_unused_mris\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "print(\n",
    "    \"  * {:,}/{:,} MRIs are scheduled for processing\".format(\n",
    "        len(raw_mris.loc[raw_mris[\"need_to_process\"] == 1]),\n",
    "        len(raw_mris),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the raw MRI scans dataframe to a CSV file\n",
    "outf = op.join(PATHS[\"scans_to_process\"], f\"Raw_MRI_Scan_Index_{now()}.csv\")\n",
    "raw_pets.to_csv(outf, index=False)\n",
    "print(f\"  * Saved raw MRI scan index to {outf}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Calculate time between serial PET scans\n",
    "raw_pets = add_pet_date_columns(raw_pets)\n",
    "print(\"  * Calculating days between scans\")\n",
    "\n",
    "# Flag PET scans with issues that preclude processing\n",
    "raw_pets = audit_pet(raw_pets)\n",
    "print(\"  * Auditing PET scans\")\n",
    "print(\n",
    "    \"    - Flagged {:,} scans with issues that may need to be resolved before processing\".format(\n",
    "        len(raw_pets.loc[raw_pets[\"flag\"] == 1])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add path to the processed PET directory\n",
    "ii = raw_pets.columns.tolist().index(\"pet_raw_niif\")\n",
    "raw_pets.insert(\n",
    "    ii + 1,\n",
    "    \"pet_proc_dir\",\n",
    "    raw_pets.apply(\n",
    "        lambda x: get_pet_proc_dir(x[\"subj\"], x[\"tracer\"], x[\"pet_date\"]), axis=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Determine which PET scans have been processed\n",
    "raw_pets.insert(\n",
    "    ii + 1, \"pet_processed\", raw_pets[\"pet_proc_dir\"].apply(check_if_pet_processed)\n",
    ")\n",
    "print(\n",
    "    \"  * {:,}/{:,} PET scans have already been processed\".format(\n",
    "        len(raw_pets.loc[raw_pets[\"pet_processed\"] == 1]),\n",
    "        len(raw_pets),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Determine which PET scans need to be processed\n",
    "raw_pets[\"need_to_process\"] = raw_pets.apply(\n",
    "    lambda x: get_pet_to_process(x[\"flag\"], x[\"pet_processed\"], overwrite), axis=1\n",
    ")\n",
    "print(\n",
    "    \"  * {:,}/{:,} PET scans are scheduled for processing\".format(\n",
    "        len(raw_pets.loc[raw_pets[\"need_to_process\"] == 1]),\n",
    "        len(raw_pets),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert PET date columns to string\n",
    "fmt = \"%Y-%m-%d\"\n",
    "for col in [\"pet_date\", \"mri_date\"]:\n",
    "    raw_pets[col] = raw_pets[col].dt.strftime(fmt)\n",
    "\n",
    "# Save the raw PET scans dataframe to a CSV file\n",
    "outf = op.join(PATHS[\"scans_to_process\"], f\"Raw_PET_Scan_Index_{now()}.csv\")\n",
    "raw_pets.to_csv(outf, index=False)\n",
    "print(f\"  * Saved raw PET scan index to {outf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_pet_processed(subj, tracer, pet_date):\n",
    "    \"\"\"Return 1 if a PET scan has been fully processed and 0 otherwise\"\"\"\n",
    "    # Check if the processed directory exists\n",
    "    processed_dir = op.join(PATHS[\"processed\"], subj, tracer, pet_date)\n",
    "    if op.isdir(processed_dir):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Loading Mayo MRI QC spreadsheet\n",
      "  * Loading MGH MRI QC spreadsheet\n"
     ]
    }
   ],
   "source": [
    "# Add MRI quality info to raw_mris\n",
    "files = glob_sort_mtime(op.join(PATHS[\"ssheets\"], \"Mayo_ADIRL_MRI_Quality*.csv\"))\n",
    "if files:\n",
    "    print(\"  * Loading Mayo MRI QC spreadsheet\")\n",
    "    mri_qc_mayof = files[0]\n",
    "    mri_qc_mayo = pd.read_csv(mri_qc_mayof)\n",
    "\n",
    "files = glob_sort_mtime(op.join(PATHS[\"ssheets\"], \"ext_mgh_mri_3t*.csv\"))\n",
    "if files:\n",
    "    print(\"  * Loading MGH MRI QC spreadsheet\")\n",
    "    mri_qc_mghf = files[0]\n",
    "    mri_qc_mgh = pd.read_csv(mri_qc_mghf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>series_date</th>\n",
       "      <th>studyinstanceuid</th>\n",
       "      <th>study_overallpass</th>\n",
       "      <th>study_comments</th>\n",
       "      <th>study_protocol_status</th>\n",
       "      <th>study_protocol_comment</th>\n",
       "      <th>study_rescan_requested</th>\n",
       "      <th>seriesinstanceuid</th>\n",
       "      <th>series_description</th>\n",
       "      <th>...</th>\n",
       "      <th>study_medical_abnormalities</th>\n",
       "      <th>serial</th>\n",
       "      <th>medical_exclusion</th>\n",
       "      <th>release_from_quarantine</th>\n",
       "      <th>pay_site</th>\n",
       "      <th>qualification</th>\n",
       "      <th>field_strength</th>\n",
       "      <th>t1_accelerated</th>\n",
       "      <th>update_stamp</th>\n",
       "      <th>mri_image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDS0370001</td>\n",
       "      <td>2018-05-09 13:07:28.0</td>\n",
       "      <td>2.16.124.113543.6006.99.8966031846000857470</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>QC'd</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.03789794833543640529</td>\n",
       "      <td>Axial 3D PASL (Eyes Open)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-09 18:05:02.0</td>\n",
       "      <td>I995988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDS0370001</td>\n",
       "      <td>2018-05-09 13:07:28.0</td>\n",
       "      <td>2.16.124.113543.6006.99.8966031846000857470</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>QC'd</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.03997603284249532785</td>\n",
       "      <td>Field Mapping</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-09 18:05:02.0</td>\n",
       "      <td>I995992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDS0370001</td>\n",
       "      <td>2018-05-09 13:07:28.0</td>\n",
       "      <td>2.16.124.113543.6006.99.8966031846000857470</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>QC'd</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.08658171394967486900</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-09 18:05:02.0</td>\n",
       "      <td>I995986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDS0370001</td>\n",
       "      <td>2018-05-09 13:07:28.0</td>\n",
       "      <td>2.16.124.113543.6006.99.8966031846000857470</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>QC'd</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.09071441183906983916</td>\n",
       "      <td>Field Mapping</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-09 18:05:02.0</td>\n",
       "      <td>I996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDS0370001</td>\n",
       "      <td>2018-05-09 13:07:28.0</td>\n",
       "      <td>2.16.124.113543.6006.99.8966031846000857470</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>QC'd</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.1453356740595980212</td>\n",
       "      <td>HighResHippocampus</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-09 18:05:02.0</td>\n",
       "      <td>I995994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16317</th>\n",
       "      <td>LDS0350597</td>\n",
       "      <td>2024-04-22 13:17:49.0</td>\n",
       "      <td>2.16.124.113543.6006.99.6815290193205908847</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Not all series passed.</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.6813834101512967121</td>\n",
       "      <td>Axial DTI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-10 18:45:32.0</td>\n",
       "      <td>I1827079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16318</th>\n",
       "      <td>LDS0350597</td>\n",
       "      <td>2024-04-22 13:17:49.0</td>\n",
       "      <td>2.16.124.113543.6006.99.6815290193205908847</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Not all series passed.</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.6813911971215949911</td>\n",
       "      <td>Axial 3TE T2 STAR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-10 18:45:32.0</td>\n",
       "      <td>I1827052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16319</th>\n",
       "      <td>LDS0350597</td>\n",
       "      <td>2024-04-22 13:17:49.0</td>\n",
       "      <td>2.16.124.113543.6006.99.6815290193205908847</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Not all series passed.</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.6814080161784333564</td>\n",
       "      <td>3 Plane Localizer</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-10 18:45:32.0</td>\n",
       "      <td>I1827078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16320</th>\n",
       "      <td>LDS0350597</td>\n",
       "      <td>2024-04-22 13:17:49.0</td>\n",
       "      <td>2.16.124.113543.6006.99.6815290193205908847</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Not all series passed.</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.6814261004355890729</td>\n",
       "      <td>Field Mapping</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-10 18:45:32.0</td>\n",
       "      <td>I1827063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16321</th>\n",
       "      <td>LDS0350597</td>\n",
       "      <td>2024-04-22 13:17:49.0</td>\n",
       "      <td>2.16.124.113543.6006.99.6815290193205908847</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Not all series passed.</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>2.16.124.113543.6006.99.6814629465303229266</td>\n",
       "      <td>Sagittal 3D FLAIR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-05-10 18:45:32.0</td>\n",
       "      <td>I1827057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16322 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id            series_date  \\\n",
       "0      LDS0370001  2018-05-09 13:07:28.0   \n",
       "1      LDS0370001  2018-05-09 13:07:28.0   \n",
       "2      LDS0370001  2018-05-09 13:07:28.0   \n",
       "3      LDS0370001  2018-05-09 13:07:28.0   \n",
       "4      LDS0370001  2018-05-09 13:07:28.0   \n",
       "...           ...                    ...   \n",
       "16317  LDS0350597  2024-04-22 13:17:49.0   \n",
       "16318  LDS0350597  2024-04-22 13:17:49.0   \n",
       "16319  LDS0350597  2024-04-22 13:17:49.0   \n",
       "16320  LDS0350597  2024-04-22 13:17:49.0   \n",
       "16321  LDS0350597  2024-04-22 13:17:49.0   \n",
       "\n",
       "                                  studyinstanceuid  study_overallpass  \\\n",
       "0      2.16.124.113543.6006.99.8966031846000857470                  1   \n",
       "1      2.16.124.113543.6006.99.8966031846000857470                  1   \n",
       "2      2.16.124.113543.6006.99.8966031846000857470                  1   \n",
       "3      2.16.124.113543.6006.99.8966031846000857470                  1   \n",
       "4      2.16.124.113543.6006.99.8966031846000857470                  1   \n",
       "...                                            ...                ...   \n",
       "16317  2.16.124.113543.6006.99.6815290193205908847                 -1   \n",
       "16318  2.16.124.113543.6006.99.6815290193205908847                 -1   \n",
       "16319  2.16.124.113543.6006.99.6815290193205908847                 -1   \n",
       "16320  2.16.124.113543.6006.99.6815290193205908847                 -1   \n",
       "16321  2.16.124.113543.6006.99.6815290193205908847                 -1   \n",
       "\n",
       "      study_comments  study_protocol_status  study_protocol_comment  \\\n",
       "0                NaN                      1                    QC'd   \n",
       "1                NaN                      1                    QC'd   \n",
       "2                NaN                      1                    QC'd   \n",
       "3                NaN                      1                    QC'd   \n",
       "4                NaN                      1                    QC'd   \n",
       "...              ...                    ...                     ...   \n",
       "16317            NaN                      3  Not all series passed.   \n",
       "16318            NaN                      3  Not all series passed.   \n",
       "16319            NaN                      3  Not all series passed.   \n",
       "16320            NaN                      3  Not all series passed.   \n",
       "16321            NaN                      3  Not all series passed.   \n",
       "\n",
       "      study_rescan_requested                             seriesinstanceuid  \\\n",
       "0                      FALSE  2.16.124.113543.6006.99.03789794833543640529   \n",
       "1                      FALSE  2.16.124.113543.6006.99.03997603284249532785   \n",
       "2                      FALSE  2.16.124.113543.6006.99.08658171394967486900   \n",
       "3                      FALSE  2.16.124.113543.6006.99.09071441183906983916   \n",
       "4                      FALSE   2.16.124.113543.6006.99.1453356740595980212   \n",
       "...                      ...                                           ...   \n",
       "16317                  FALSE   2.16.124.113543.6006.99.6813834101512967121   \n",
       "16318                  FALSE   2.16.124.113543.6006.99.6813911971215949911   \n",
       "16319                  FALSE   2.16.124.113543.6006.99.6814080161784333564   \n",
       "16320                  FALSE   2.16.124.113543.6006.99.6814261004355890729   \n",
       "16321                  FALSE   2.16.124.113543.6006.99.6814629465303229266   \n",
       "\n",
       "                series_description  ... study_medical_abnormalities  serial  \\\n",
       "0        Axial 3D PASL (Eyes Open)  ...                         NaN     NaN   \n",
       "1                    Field Mapping  ...                         NaN     NaN   \n",
       "2      Accelerated Sagittal MPRAGE  ...                         NaN     NaN   \n",
       "3                    Field Mapping  ...                         NaN     NaN   \n",
       "4               HighResHippocampus  ...                         NaN     NaN   \n",
       "...                            ...  ...                         ...     ...   \n",
       "16317                    Axial DTI  ...                         NaN     NaN   \n",
       "16318            Axial 3TE T2 STAR  ...                         NaN     NaN   \n",
       "16319            3 Plane Localizer  ...                         NaN     NaN   \n",
       "16320                Field Mapping  ...                         NaN     NaN   \n",
       "16321            Sagittal 3D FLAIR  ...                         NaN     NaN   \n",
       "\n",
       "      medical_exclusion  release_from_quarantine pay_site  qualification  \\\n",
       "0                   NaN                      1.0      NaN              0   \n",
       "1                   NaN                      1.0      NaN              0   \n",
       "2                   NaN                      1.0      NaN              0   \n",
       "3                   NaN                      1.0      NaN              0   \n",
       "4                   NaN                      1.0      NaN              0   \n",
       "...                 ...                      ...      ...            ...   \n",
       "16317               NaN                      NaN      NaN              0   \n",
       "16318               NaN                      NaN      NaN              0   \n",
       "16319               NaN                      NaN      NaN              0   \n",
       "16320               NaN                      NaN      NaN              0   \n",
       "16321               NaN                      NaN      NaN              0   \n",
       "\n",
       "       field_strength  t1_accelerated           update_stamp  mri_image_id  \n",
       "0                 3.0             NaN  2021-02-09 18:05:02.0       I995988  \n",
       "1                 3.0             NaN  2021-02-09 18:05:02.0       I995992  \n",
       "2                 3.0             NaN  2021-02-09 18:05:02.0       I995986  \n",
       "3                 3.0             NaN  2021-02-09 18:05:02.0       I996000  \n",
       "4                 3.0             NaN  2021-02-09 18:05:02.0       I995994  \n",
       "...               ...             ...                    ...           ...  \n",
       "16317             3.0             NaN  2024-05-10 18:45:32.0      I1827079  \n",
       "16318             3.0             NaN  2024-05-10 18:45:32.0      I1827052  \n",
       "16319             3.0             NaN  2024-05-10 18:45:32.0      I1827078  \n",
       "16320             3.0             NaN  2024-05-10 18:45:32.0      I1827063  \n",
       "16321             3.0             NaN  2024-05-10 18:45:32.0      I1827057  \n",
       "\n",
       "[16322 rows x 29 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rename_cols = {\n",
    "    \"subject_id\": \"subj\",\n",
    "    \"series_date\": \"mri_date\",\n",
    "    \"study_overallpass\": \"mayo_pass\",\n",
    "    \"series_quality\": \"mayo_quality\",\n",
    "    \"series_comments\": \"mayo_comments\",\n",
    "}\n",
    "mri_qc_mayo.rename(\n",
    "    columns={\n",
    "        \"subject_id\": \"subj\",\n",
    "        \"series_date\": \"mri_date\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_code</th>\n",
       "      <th>download_date</th>\n",
       "      <th>run_number</th>\n",
       "      <th>loni_image</th>\n",
       "      <th>loni_series</th>\n",
       "      <th>loni_study</th>\n",
       "      <th>coil</th>\n",
       "      <th>Description</th>\n",
       "      <th>Acq Date</th>\n",
       "      <th>OVERALLQC</th>\n",
       "      <th>...</th>\n",
       "      <th>lh_rostralmiddlefrontal_thicknessstd_aparc</th>\n",
       "      <th>lh_superiorfrontal_thicknessstd_aparc</th>\n",
       "      <th>lh_superiorparietal_thicknessstd_aparc</th>\n",
       "      <th>lh_superiortemporal_thicknessstd_aparc</th>\n",
       "      <th>lh_supramarginal_thicknessstd_aparc</th>\n",
       "      <th>lh_frontalpole_thicknessstd_aparc</th>\n",
       "      <th>lh_temporalpole_thicknessstd_aparc</th>\n",
       "      <th>lh_transversetemporal_thicknessstd_aparc</th>\n",
       "      <th>lh_insula_thicknessstd_aparc</th>\n",
       "      <th>update_stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDS0370001</td>\n",
       "      <td>12/19/2018</td>\n",
       "      <td>2</td>\n",
       "      <td>995986</td>\n",
       "      <td>684237.0</td>\n",
       "      <td>122553.0</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>5/9/2018</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.823</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDS0370002</td>\n",
       "      <td>3/20/2023</td>\n",
       "      <td>2</td>\n",
       "      <td>1006346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>6/6/2018</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.728</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDS0370006</td>\n",
       "      <td>12/17/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>1026569</td>\n",
       "      <td>708999.0</td>\n",
       "      <td>125027.0</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>7/26/2018</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.793</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDS0370007</td>\n",
       "      <td>11/9/2021</td>\n",
       "      <td>2</td>\n",
       "      <td>1029698</td>\n",
       "      <td>711584.0</td>\n",
       "      <td>125258.0</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>8/1/2018</td>\n",
       "      <td>Pass</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.713</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDS0370005</td>\n",
       "      <td>12/19/2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1030751</td>\n",
       "      <td>712380.0</td>\n",
       "      <td>125364.0</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>8/2/2018</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.723</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>LDS3600602</td>\n",
       "      <td>9/29/2021</td>\n",
       "      <td>2</td>\n",
       "      <td>1663804</td>\n",
       "      <td>1193184.0</td>\n",
       "      <td>222555.0</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>2/8/2023</td>\n",
       "      <td>Pass</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.889</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>LDS9410184</td>\n",
       "      <td>4/1/2023</td>\n",
       "      <td>2</td>\n",
       "      <td>1681240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>3/22/2023</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.670</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>LDS9410568</td>\n",
       "      <td>4/1/2023</td>\n",
       "      <td>2</td>\n",
       "      <td>1646937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>12/2/2022</td>\n",
       "      <td>Pass</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.794</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>LDS9410572</td>\n",
       "      <td>4/1/2023</td>\n",
       "      <td>2</td>\n",
       "      <td>1647499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>12/5/2022</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.689</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>LDS9410608</td>\n",
       "      <td>3/20/2023</td>\n",
       "      <td>2</td>\n",
       "      <td>1664985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20ch</td>\n",
       "      <td>Accelerated Sagittal MPRAGE</td>\n",
       "      <td>2/10/2023</td>\n",
       "      <td>Pass</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.878</td>\n",
       "      <td>2023-07-02 21:24:01.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>870 rows × 460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_code download_date  run_number  loni_image  loni_series  \\\n",
       "0     LDS0370001    12/19/2018           2      995986     684237.0   \n",
       "1     LDS0370002     3/20/2023           2     1006346          NaN   \n",
       "2     LDS0370006    12/17/2019           5     1026569     708999.0   \n",
       "3     LDS0370007     11/9/2021           2     1029698     711584.0   \n",
       "4     LDS0370005    12/19/2018           2     1030751     712380.0   \n",
       "..           ...           ...         ...         ...          ...   \n",
       "865   LDS3600602     9/29/2021           2     1663804    1193184.0   \n",
       "866   LDS9410184      4/1/2023           2     1681240          NaN   \n",
       "867   LDS9410568      4/1/2023           2     1646937          NaN   \n",
       "868   LDS9410572      4/1/2023           2     1647499          NaN   \n",
       "869   LDS9410608     3/20/2023           2     1664985          NaN   \n",
       "\n",
       "     loni_study  coil                  Description   Acq Date OVERALLQC  ...  \\\n",
       "0      122553.0  64ch  Accelerated Sagittal MPRAGE   5/9/2018   Partial  ...   \n",
       "1           NaN  64ch  Accelerated Sagittal MPRAGE   6/6/2018   Partial  ...   \n",
       "2      125027.0  64ch  Accelerated Sagittal MPRAGE  7/26/2018   Partial  ...   \n",
       "3      125258.0  64ch  Accelerated Sagittal MPRAGE   8/1/2018      Pass  ...   \n",
       "4      125364.0  64ch  Accelerated Sagittal MPRAGE   8/2/2018   Partial  ...   \n",
       "..          ...   ...                          ...        ...       ...  ...   \n",
       "865    222555.0  64ch  Accelerated Sagittal MPRAGE   2/8/2023      Pass  ...   \n",
       "866         NaN  64ch  Accelerated Sagittal MPRAGE  3/22/2023   Partial  ...   \n",
       "867         NaN  64ch  Accelerated Sagittal MPRAGE  12/2/2022      Pass  ...   \n",
       "868         NaN  64ch  Accelerated Sagittal MPRAGE  12/5/2022   Partial  ...   \n",
       "869         NaN  20ch  Accelerated Sagittal MPRAGE  2/10/2023      Pass  ...   \n",
       "\n",
       "    lh_rostralmiddlefrontal_thicknessstd_aparc  \\\n",
       "0                                        0.500   \n",
       "1                                        0.405   \n",
       "2                                        0.467   \n",
       "3                                        0.491   \n",
       "4                                        0.472   \n",
       "..                                         ...   \n",
       "865                                      0.528   \n",
       "866                                      0.401   \n",
       "867                                      0.459   \n",
       "868                                      0.484   \n",
       "869                                      0.531   \n",
       "\n",
       "    lh_superiorfrontal_thicknessstd_aparc  \\\n",
       "0                                   0.533   \n",
       "1                                   0.411   \n",
       "2                                   0.519   \n",
       "3                                   0.487   \n",
       "4                                   0.469   \n",
       "..                                    ...   \n",
       "865                                 0.563   \n",
       "866                                 0.464   \n",
       "867                                 0.490   \n",
       "868                                 0.518   \n",
       "869                                 0.527   \n",
       "\n",
       "    lh_superiorparietal_thicknessstd_aparc  \\\n",
       "0                                    0.443   \n",
       "1                                    0.335   \n",
       "2                                    0.433   \n",
       "3                                    0.433   \n",
       "4                                    0.419   \n",
       "..                                     ...   \n",
       "865                                  0.506   \n",
       "866                                  0.481   \n",
       "867                                  0.423   \n",
       "868                                  0.423   \n",
       "869                                  0.423   \n",
       "\n",
       "    lh_superiortemporal_thicknessstd_aparc  \\\n",
       "0                                    0.592   \n",
       "1                                    0.517   \n",
       "2                                    0.579   \n",
       "3                                    0.573   \n",
       "4                                    0.578   \n",
       "..                                     ...   \n",
       "865                                  0.578   \n",
       "866                                  0.545   \n",
       "867                                  0.526   \n",
       "868                                  0.607   \n",
       "869                                  0.542   \n",
       "\n",
       "    lh_supramarginal_thicknessstd_aparc lh_frontalpole_thicknessstd_aparc  \\\n",
       "0                                 0.514                             0.538   \n",
       "1                                 0.408                             0.333   \n",
       "2                                 0.461                             0.448   \n",
       "3                                 0.434                             0.454   \n",
       "4                                 0.472                             0.391   \n",
       "..                                  ...                               ...   \n",
       "865                               0.465                             0.551   \n",
       "866                               0.425                             0.391   \n",
       "867                               0.469                             0.501   \n",
       "868                               0.421                             0.594   \n",
       "869                               0.450                             0.458   \n",
       "\n",
       "    lh_temporalpole_thicknessstd_aparc  \\\n",
       "0                                0.701   \n",
       "1                                0.565   \n",
       "2                                0.797   \n",
       "3                                0.744   \n",
       "4                                0.809   \n",
       "..                                 ...   \n",
       "865                              0.604   \n",
       "866                              0.710   \n",
       "867                              0.760   \n",
       "868                              0.571   \n",
       "869                              0.866   \n",
       "\n",
       "    lh_transversetemporal_thicknessstd_aparc lh_insula_thicknessstd_aparc  \\\n",
       "0                                      0.360                        0.823   \n",
       "1                                      0.318                        0.728   \n",
       "2                                      0.362                        0.793   \n",
       "3                                      0.435                        0.713   \n",
       "4                                      0.338                        0.723   \n",
       "..                                       ...                          ...   \n",
       "865                                    0.285                        0.889   \n",
       "866                                    0.432                        0.670   \n",
       "867                                    0.440                        0.794   \n",
       "868                                    0.388                        0.689   \n",
       "869                                    0.302                        0.878   \n",
       "\n",
       "              update_stamp  \n",
       "0    2023-07-02 21:24:01.0  \n",
       "1    2023-07-02 21:24:01.0  \n",
       "2    2023-07-02 21:24:01.0  \n",
       "3    2023-07-02 21:24:01.0  \n",
       "4    2023-07-02 21:24:01.0  \n",
       "..                     ...  \n",
       "865  2023-07-02 21:24:01.0  \n",
       "866  2023-07-02 21:24:01.0  \n",
       "867  2023-07-02 21:24:01.0  \n",
       "868  2023-07-02 21:24:01.0  \n",
       "869  2023-07-02 21:24:01.0  \n",
       "\n",
       "[870 rows x 460 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mri_qc_mayo[\"mri_image_id\"] = mri_qc_mayo[\"loni_image\"].apply(lambda x: f\"I{x}\")\n",
    "mri_qc_mgh[\"mri_image_id\"] = mri_qc_mgh[\"loni_image\"].apply(lambda x: f\"I{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>mri_date</th>\n",
       "      <th>mri_image_id</th>\n",
       "      <th>mri_raw_niif</th>\n",
       "      <th>mri_used_for_pet_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>I1467558</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subj    mri_date mri_image_id  \\\n",
       "1  LDS0070120  2021-07-13     I1467558   \n",
       "\n",
       "                                        mri_raw_niif  mri_used_for_pet_proc  \n",
       "1  /mnt/coredata/processing/leads/data/raw/LDS007...                   True  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_mris.query(\"(mri_image_id=='I1467558')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pet_raw_niif\n",
       "6    2066\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pets[\"pet_raw_niif\"].apply(get_pet_resolution).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pets[\"pet_res\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>tracer</th>\n",
       "      <th>mri_image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>LDS0220071</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>LDS0220071</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>LDS0350342</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>LDS0360283</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>LDS0370038</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>LDS0730083</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subj tracer  mri_image_id\n",
       "237  LDS0220071    FBB             2\n",
       "238  LDS0220071    FTP             2\n",
       "331  LDS0350342    FBB             2\n",
       "374  LDS0360283    FTP             2\n",
       "488  LDS0370038    FBB             2\n",
       "750  LDS0730083    FTP             2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pets.groupby([\"subj\", \"tracer\"])[\"mri_image_id\"].apply(\n",
    "    lambda x: np.max(np.unique(x, return_counts=True)[1])\n",
    ").reset_index().query(\"(mri_image_id!=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge raw_pets with raw_mris\n",
    "# dxf = op.join(PATHS[\"metadata\"], \"ssheets\", \"LEADS_Internal_PET-Screening.xlsx\")\n",
    "# if op.isfile(dxf):\n",
    "#     dx = pd.read_excel(dxf)\n",
    "#     dx_map = {\"ID\": \"subj\", \"Cohort\": \"dx\"}\n",
    "#     dx = dx.rename(columns=dx_map)[[\"subj\", \"dx\"]]\n",
    "#     raw_scans = dx.merge(raw_scans, on=\"subj\", how=\"right\")\n",
    "\n",
    "# # Add controls.\n",
    "# subj_regf = op.join(\n",
    "#     PATHS[\"metadata\"], \"ssheets\", \"Participant Registration_vertical.csv\"\n",
    "# )\n",
    "# if op.isfile(subj_regf):\n",
    "#     subj_reg = pd.read_csv(subj_regf)\n",
    "#     subj_reg_map = {\"subject.label\": \"subj\", \"dd_revision_field.translated_value\": \"dx\"}\n",
    "#     subj_reg = subj_reg.rename(columns=subj_reg_map)[[\"subj\", \"dx\"]]\n",
    "# cn_subjs = subj_reg.query(\"(dx=='Cognitively Normal Participant')\")[\"subj\"].tolist()\n",
    "# raw_scans.loc[pd.isna(raw_scans[\"dx\"]), \"dx\"] = raw_scans.loc[\n",
    "#     pd.isna(raw_scans[\"dx\"]), \"subj\"\n",
    "# ].apply(lambda x: \"CN\" if np.isin(x, cn_subjs) else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2069 scans from 619 subjects in /mnt/coredata/processing/leads/data/raw\n",
      "WARNING: LDS9410459 scan on 2024-03-05 has no matching MRI within 365 days\n",
      "WARNING: LDS0220071 scan on 2022-08-09 has no matching MRI within 365 days\n",
      "WARNING: LDS0370038 scan on 2023-01-05 has no matching MRI within 365 days\n",
      "WARNING: LDS0350171 scan on 2023-11-01 has no matching MRI within 365 days\n",
      "WARNING: LDS9410323 scan on 2020-12-10 has no processed MRI scans in /mnt/coredata/processing/leads/data/freesurfer\n",
      "WARNING: LDS0350342 scan on 2022-08-31 has no matching MRI within 365 days\n",
      "WARNING: LDS0370015 scan on 2022-05-10 has no matching MRI within 365 days\n",
      "WARNING: LDS0370316 scan on 2024-02-15 has no matching MRI within 365 days\n",
      "WARNING: LDS0350171 scan on 2023-11-30 has no matching MRI within 365 days\n",
      "WARNING: LDS0370449 scan on 2024-03-11 has no matching MRI within 365 days\n",
      "WARNING: LDS0220071 scan on 2022-10-18 has no matching MRI within 365 days\n",
      "WARNING: LDS0730083 scan on 2021-01-19 has no matching MRI within 365 days\n",
      "WARNING: LDS9410323 scan on 2021-01-07 has no processed MRI scans in /mnt/coredata/processing/leads/data/freesurfer\n",
      "WARNING: LDS0350342 scan on 2022-09-21 has no matching MRI within 365 days\n",
      "raw_scans: (2069, 7)\n"
     ]
    }
   ],
   "source": [
    "# Scrape the raw directory for all PET niftis\n",
    "raw_scans = scrape_raw(PATHS[\"raw\"])\n",
    "\n",
    "# Get paths for the raw nifti files copied into the processed directory\n",
    "# raw_cp_str = op.join(\n",
    "#     PATHS[\"processed\"],\n",
    "#     \"{subj}\",\n",
    "#     \"{scan_type}_{scan_date}\",\n",
    "#     \"{subj}_{scan_type}_{scan_date}.nii\",\n",
    "# )\n",
    "\n",
    "# Search processed Freesurfer dirs for closest MRI to each PET scan\n",
    "raw_scans[\"mri_date\"], raw_scans[\"days_to_mri\"], raw_scans[\"fs_dir\"] = zip(\n",
    "    *raw_scans.apply(\n",
    "        lambda x: find_closest_mri(x[\"subj\"], x[\"scan_date\"], PATHS[\"freesurfer\"]),\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"raw_scans: {raw_scans.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/coredata/Projects/LEADS/data_f7p1/freesurfer_processing/LDS0370001_MRI_T1_2020-10-14'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = op.join(\n",
    "    \"/mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0370001/Timepoint2/MRI_T1_2020-10-14\",\n",
    "    \"LDS0370001_MRI_T1_2020-10-14_nu.nii\",\n",
    ")\n",
    "op.islink(fname)\n",
    "freesurfer_scan_dir = op.dirname(op.dirname(op.abspath(os.readlink(fname))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add processed MRI directories to raw_scans\n",
    "# proc_dir_old = \"/mnt/coredata/Projects/LEADS/data_f7p1/processed\"\n",
    "# mri_dirs = []\n",
    "# proc_mri_dirs_old = []\n",
    "# freesurfer_dirs_old = []\n",
    "# for idx, scan in raw_scans.iterrows():\n",
    "#     if scan[\"fs_dir\"] is np.nan:\n",
    "#         mri_dirs.append(np.nan)\n",
    "#         continue\n",
    "\n",
    "#     subj = scan[\"subj\"]\n",
    "#     mri_date = scan[\"fs_dir\"].split(\"_\")[1]\n",
    "#     proc_mri_dir_new = op.join(PATHS[\"processed\"], subj, f\"MRI-T1_{mri_date}\")\n",
    "\n",
    "#     # Find the old processed MRI and FreeSurfer directories\n",
    "#     max_tp = 6\n",
    "#     for tp in range(1, max_tp + 1):\n",
    "#         proc_mri_dir_old = op.join(\n",
    "#             proc_dir_old, subj, f\"Timepoint{tp}\", f\"MRI_T1_{mri_date}\"\n",
    "#         )\n",
    "#         if op.isdir(proc_mri_dir_old):\n",
    "#             os.makedirs(op.dirname(proc_mri_dir_new), exist_ok=True)\n",
    "#             # if not op.exists(proc_mri_dir_new):\n",
    "#             #     os.symlink(proc_mri_dir_old, proc_mri_dir_new)\n",
    "#             if op.islink(proc_mri_dir_new):\n",
    "#                 os.unlink(proc_mri_dir_new)\n",
    "#             os.makedirs(proc_mri_dir_new, exist_ok=True)\n",
    "#             proc_mri_dirs_old.append(proc_mri_dir_old)\n",
    "\n",
    "#             # Find the FreeSurfer directory from the nu.nii symlink\n",
    "#             nu_old = op.join(proc_mri_dir_old, f\"{subj}_MRI_T1_{mri_date}_nu.nii\")\n",
    "#             if op.islink(nu_old):\n",
    "#                 freesurfer_dir_old = op.dirname(\n",
    "#                     op.dirname(op.abspath(os.readlink(fname)))\n",
    "#                 )\n",
    "#                 freesurfer_dir_new = op.join(proc_mri_dir_new, \"freesurfer_7p1\")\n",
    "#                 freesurfer_link_new = op.join(proc_mri_dir_new, \"freesurfer\")\n",
    "#                 shutil.copytree(freesurfer_scan_dir, proc_mri_dir_new)\n",
    "#             break\n",
    "#         if tp == max_tp:\n",
    "#             proc_mri_dir_old = np.nan\n",
    "\n",
    "#     # Add the processed MRI directory to raw_scans[\"mri_dir\"]\n",
    "#     if op.exists(proc_mri_dir_new):\n",
    "#         mri_dirs.append(proc_mri_dir_new)\n",
    "#     else:\n",
    "#         mri_dirs.append(np.nan)\n",
    "\n",
    "# raw_scans[\"mri_dir\"] = mri_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_scans: (2067, 9)\n"
     ]
    }
   ],
   "source": [
    "# Drop raw_scans rows with missing data, then sort rows.\n",
    "raw_scans = (\n",
    "    raw_scans.dropna()\n",
    "    .sort_values([\"subj\", \"scan_type\", \"scan_date\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Convert days_to_mri to int\n",
    "raw_scans[\"days_to_mri\"] = raw_scans[\"days_to_mri\"].astype(int)\n",
    "\n",
    "# Add a visit column to raw_scans, with visit 1 being the earliest date\n",
    "# for a given subject and scan_type, visit 2 being the next earliest\n",
    "# date, and so on.\n",
    "raw_scans[\"visit\"] = raw_scans.groupby([\"subj\", \"scan_type\"]).cumcount() + 1\n",
    "cols = raw_scans.columns.tolist()\n",
    "cols.insert(cols.index(\"scan_type\") + 1, cols.pop(cols.index(\"visit\")))\n",
    "raw_scans = raw_scans[cols]\n",
    "\n",
    "print(f\"raw_scans: {raw_scans.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a diagnosis column to raw_scans\n",
    "dxf = op.join(PATHS[\"metadata\"], \"ssheets\", \"LEADS_Internal_PET-Screening.xlsx\")\n",
    "if op.isfile(dxf):\n",
    "    dx = pd.read_excel(dxf)\n",
    "    dx_map = {\"ID\": \"subj\", \"Cohort\": \"dx\"}\n",
    "    dx = dx.rename(columns=dx_map)[[\"subj\", \"dx\"]]\n",
    "    raw_scans = dx.merge(raw_scans, on=\"subj\", how=\"right\")\n",
    "\n",
    "# Add controls.\n",
    "subj_regf = op.join(\n",
    "    PATHS[\"metadata\"], \"ssheets\", \"Participant Registration_vertical.csv\"\n",
    ")\n",
    "if op.isfile(subj_regf):\n",
    "    subj_reg = pd.read_csv(subj_regf)\n",
    "    subj_reg_map = {\"subject.label\": \"subj\", \"dd_revision_field.translated_value\": \"dx\"}\n",
    "    subj_reg = subj_reg.rename(columns=subj_reg_map)[[\"subj\", \"dx\"]]\n",
    "cn_subjs = subj_reg.query(\"(dx=='Cognitively Normal Participant')\")[\"subj\"].tolist()\n",
    "raw_scans.loc[pd.isna(raw_scans[\"dx\"]), \"dx\"] = raw_scans.loc[\n",
    "    pd.isna(raw_scans[\"dx\"]), \"subj\"\n",
    "].apply(lambda x: \"CN\" if np.isin(x, cn_subjs) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find PET scans that are ready and needing to be processed\n",
    "proc_pet_dirs = []\n",
    "need_to_process = []\n",
    "for idx, scan in raw_scans.iterrows():\n",
    "    if scan[\"mri_dir\"] is np.nan:\n",
    "        proc_pet_dirs.append(np.nan)\n",
    "        need_to_process.append(False)\n",
    "        continue\n",
    "\n",
    "    proc_pet_dir = op.join(\n",
    "        PATHS[\"processed\"], scan[\"subj\"], f\"{scan['scan_type']}_{scan['scan_date']}\"\n",
    "    )\n",
    "    proc_pet_dirs.append(proc_pet_dir)\n",
    "    need_to_process.append(not op.exists(proc_pet_dir))\n",
    "\n",
    "raw_scans[\"proc_pet_dir\"] = proc_pet_dirs\n",
    "raw_scans[\"need_to_process\"] = need_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw_scans to /mnt/coredata/processing/leads/metadata/log/raw_pet_scans_2024-04-22-19-12-02.csv\n"
     ]
    }
   ],
   "source": [
    "# Save raw_scans to a CSV file\n",
    "outf = op.join(PATHS[\"metadata\"], \"log\", f\"raw_pet_scans_{now()}.csv\")\n",
    "raw_scans.to_csv(outf, index=False)\n",
    "print(f\"Saved raw_scans to {outf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy MRI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_freesurfer(scan, rm_existing=False):\n",
    "    \"\"\"Copy FreeSurfer directory to the processed MRI directory.\"\"\"\n",
    "    # Add processed MRI directories to raw_scans\n",
    "    fs_dir_old = \"/mnt/coredata/Projects/LEADS/data_f7p1/freesurfer_processing\"\n",
    "\n",
    "    # Create the processed MRI directory, if it doesn't already exist\n",
    "    if op.islink(scan[\"mri_dir\"]):\n",
    "        os.unlink(scan[\"mri_dir\"])\n",
    "    os.makedirs(scan[\"mri_dir\"], exist_ok=True)\n",
    "\n",
    "    # Copy the FreeSurfer directory to the processed MRI directory\n",
    "    fs_scan_dir_old = op.join(fs_dir_old, f\"{scan['subj']}_MRI_T1_{scan['mri_date']}\")\n",
    "    fs_scan_dir_new = op.join(scan[\"mri_dir\"], f\"freesurfer_7p1\")\n",
    "    fs_scan_link_new = fs_scan_dir_new.replace(\"freesurfer_7p1\", \"freesurfer\")\n",
    "    if op.isdir(fs_scan_dir_old):\n",
    "        if op.isdir(fs_scan_dir_new) and rm_existing:\n",
    "            shutil.rmtree(fs_scan_dir_new)\n",
    "        if not op.isdir(fs_scan_dir_new):\n",
    "            # Find files in fs_scan_dir_old that are dirs\n",
    "            subfiles = [\n",
    "                f\n",
    "                for f in os.listdir(fs_scan_dir_old)\n",
    "                if not op.islink(op.join(fs_scan_dir_old, f))\n",
    "            ]\n",
    "            if len(subfiles) > 0:\n",
    "                os.makedirs(fs_scan_dir_new, exist_ok=True)\n",
    "                for f in subfiles:\n",
    "                    shutil.copytree(\n",
    "                        op.join(fs_scan_dir_old, f),\n",
    "                        op.join(fs_scan_dir_new, f),\n",
    "                        symlinks=True,\n",
    "                    )\n",
    "\n",
    "            # Add a symlink to the new freesurfer directory\n",
    "            if op.islink(fs_scan_link_new):\n",
    "                os.unlink(fs_scan_link_new)\n",
    "            os.symlink(fs_scan_dir_new, fs_scan_link_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying FreeSurfer directories for 1092 MRIs\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "max_workers = 16\n",
    "mris_to_copy = raw_scans.drop_duplicates([\"subj\", \"mri_date\"]).to_dict(orient=\"records\")\n",
    "print(f\"Copying FreeSurfer directories for {len(mris_to_copy)} MRIs\")\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    executor.map(copy_freesurfer, mris_to_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add processed MRI directories to raw_scans\n",
    "cp_freesurfer = True\n",
    "fs_dir_old = \"/mnt/coredata/Projects/LEADS/data_f7p1/freesurfer_processing\"\n",
    "for idx, scan in raw_scans.drop_duplicates([\"subj\", \"mri_date\"]).iterrows():\n",
    "    if scan[\"subj\"] != \"LDS0370008\":\n",
    "        continue\n",
    "\n",
    "    # Create the processed MRI directory, if it doesn't already exist\n",
    "    if op.islink(scan[\"mri_dir\"]):\n",
    "        os.unlink(scan[\"mri_dir\"])\n",
    "    os.makedirs(scan[\"mri_dir\"], exist_ok=True)\n",
    "\n",
    "    # Copy the FreeSurfer directory to the processed MRI directory\n",
    "    fs_scan_dir_old = op.join(fs_dir_old, f\"{scan['subj']}_MRI_T1_{scan['mri_date']}\")\n",
    "    fs_scan_dir_new = op.join(scan[\"mri_dir\"], f\"freesurfer_7p1\")\n",
    "    fs_scan_link_new = fs_scan_dir_new.replace(\"freesurfer_7p1\", \"freesurfer\")\n",
    "    if op.isdir(fs_scan_dir_old):\n",
    "        if cp_freesurfer and not op.isdir(fs_scan_dir_new):\n",
    "            # Find files in fs_scan_dir_old that are dirs\n",
    "            subfiles = [\n",
    "                f\n",
    "                for f in os.listdir(fs_scan_dir_old)\n",
    "                if not op.islink(op.join(fs_scan_dir_old, f))\n",
    "            ]\n",
    "            if len(subfiles) > 0:\n",
    "                os.makedirs(fs_scan_dir_new, exist_ok=True)\n",
    "                for f in subfiles:\n",
    "                    shutil.copytree(\n",
    "                        op.join(fs_scan_dir_old, f),\n",
    "                        op.join(fs_scan_dir_new, f),\n",
    "                        symlinks=True,\n",
    "                    )\n",
    "\n",
    "            # Add a symlink to the new freesurfer directory\n",
    "            if op.islink(fs_scan_link_new):\n",
    "                os.unlink(fs_scan_link_new)\n",
    "            os.symlink(fs_scan_dir_new, fs_scan_link_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate dirs in raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/coredata/processing/leads/data/raw'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirs = [\"fbb\", \"fdg\", \"ftp\", \"mri\"]\n",
    "# for d in subdirs:\n",
    "#     os.makedirs(op.join(PATHS[\"processed\"], d), exist_ok=True)\n",
    "PATHS[\"raw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move raw scan directories from raw/<scan_type>/<subj> to raw/<subj>\n",
    "scan_type_dirs = [\n",
    "    op.join(PATHS[\"raw\"], f)\n",
    "    for f in os.listdir(PATHS[\"raw\"])\n",
    "    if op.isdir(op.join(PATHS[\"raw\"], f))\n",
    "]\n",
    "for scan_type_dir in scan_type_dirs:\n",
    "    subj_dirs_old = [\n",
    "        op.join(scan_type_dir, f)\n",
    "        for f in os.listdir(scan_type_dir)\n",
    "        if op.isdir(op.join(scan_type_dir, f))\n",
    "    ]\n",
    "    for subj_dir_old in subj_dirs_old:\n",
    "        subj_dir_new = op.join(PATHS[\"raw\"], op.basename(subj_dir_old))\n",
    "        subj_scan_dirs_old = [\n",
    "            op.join(subj_dir_old, f)\n",
    "            for f in os.listdir(subj_dir_old)\n",
    "            if op.isdir(op.join(subj_dir_old, f))\n",
    "        ]\n",
    "        # Make the new subject directory if it doesn't already exist\n",
    "        if subj_scan_dirs_old:\n",
    "            os.makedirs(subj_dir_new, exist_ok=True)\n",
    "        for subj_scan_dir_old in subj_scan_dirs_old:\n",
    "            # Move the scan directory to its new location\n",
    "            shutil.move(subj_scan_dir_old, subj_dir_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_newdata_to_raw(\n",
    "    newdata_dir, raw_dir, overwrite=False, cleanup=True, verbose=True\n",
    "):\n",
    "    \"\"\"Move scans from newdata to raw, keeping file hierarchies intact.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    newdata_dir : str\n",
    "        The directory containing the new scan data. Must format like:\n",
    "            <newdata_dir>/<subj>/<...>/<nifti_or_dicom_files>\n",
    "    raw_dir : str\n",
    "        The directory to move the new scan data to. Structure after\n",
    "        moving will be:\n",
    "            <raw_dir>/<subj>/<...>/<nifti_or_dicom_files>\n",
    "    overwrite : bool\n",
    "        If True, overwrite existing scan directories in raw_dir with\n",
    "        directories from newdata_dir. If False, skip existing\n",
    "        directories.\n",
    "    cleanup : bool\n",
    "        If True, remove all files and folders from newdata_dir after\n",
    "        moving everything eligible to be moved to raw_dir.\n",
    "    verbose : bool\n",
    "        If True, print messages about what is happening as the function\n",
    "        runs.\n",
    "    \"\"\"\n",
    "\n",
    "    def do_cleanup():\n",
    "        \"\"\"Remove all files and folders from newdata.\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"  Cleaning up {newdata_dir}\")\n",
    "        for file in os.listdir(newdata_dir):\n",
    "            filepath = op.join(newdata_dir, file)\n",
    "            if op.isdir(filepath):\n",
    "                shutil.rmtree(filepath)\n",
    "            else:\n",
    "                os.remove(filepath)\n",
    "\n",
    "    # Ensure the base directory paths are absolute and normalized\n",
    "    newdata_dir = op.normpath(newdata_dir)\n",
    "    raw_dir = op.normpath(raw_dir)\n",
    "\n",
    "    # Find all nifti and dicom files in newdata\n",
    "    check_exts = (\".nii\", \".nii.gz\", \".IMA\", \".dcm\")\n",
    "    glob_files = []\n",
    "    for ext in check_exts:\n",
    "        glob_files.extend(glob(op.join(newdata_dir, f\"**/*{ext}\"), recursive=True))\n",
    "\n",
    "    if verbose:\n",
    "        title = \"Moving newdata to raw\"\n",
    "        print(title, \"-\" * len(title), sep=\"\\n\")\n",
    "    if len(glob_files) == 0:\n",
    "        if verbose:\n",
    "            print(f\"  No nifti or dicom files found in {newdata_dir}\")\n",
    "        do_cleanup()\n",
    "        return\n",
    "\n",
    "    # Find all unique nifti- or dicom-containing directories in newdata\n",
    "    source_dirs = set([op.dirname(f) for f in glob_files])\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"  Found {len(source_dirs)} nifti- or dicom-containing directories in {newdata_dir}\"\n",
    "        )\n",
    "\n",
    "    for source_dir in source_dirs:\n",
    "        # Create a matching file hierarchy in raw as in newdata\n",
    "        target_dir = op.join(raw_dir, op.relpath(source_dir, newdata_dir))\n",
    "\n",
    "        # Check if the target directory exists\n",
    "        if op.exists(target_dir):\n",
    "            # If overwrite is True, remove the existing directory\n",
    "            if overwrite:\n",
    "                if verbose:\n",
    "                    print(f\"  Overwriting existing raw directory: {target_dir}\")\n",
    "                shutil.rmtree(target_dir)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  Skipping existing raw directory: {target_dir}\")\n",
    "                continue\n",
    "\n",
    "        # Create the necessary directory structure, then copy source to\n",
    "        # target\n",
    "        os.makedirs(op.dirname(target_dir), exist_ok=True)\n",
    "        shutil.move(source_dir, target_dir)\n",
    "        if verbose:\n",
    "            print(f\"  Moved {source_dir} to {target_dir}\")\n",
    "\n",
    "    # Clean up empty directories in newdata\n",
    "    if cleanup:\n",
    "        do_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving newdata to raw\n",
      "---------------------\n",
      "  Found 7 nifti- or dicom-containing directories in /home/mac/dschonhaut/tmp/leads/restructuring/newdata\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/b/aa\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/a\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/c/aa/aaa\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/c/aa/aab\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/b/ab\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/c/ab/aab\n",
      "  Skipping existing raw directory: /home/mac/dschonhaut/tmp/leads/restructuring/raw/c/ab/aaa\n",
      "  Cleaning up /home/mac/dschonhaut/tmp/leads/restructuring/newdata\n"
     ]
    }
   ],
   "source": [
    "newdata_dir = \"/home/mac/dschonhaut/tmp/leads/restructuring/newdata\"\n",
    "raw_dir = \"/home/mac/dschonhaut/tmp/leads/restructuring/raw\"\n",
    "\n",
    "move_newdata_to_raw(\n",
    "    newdata_dir=newdata_dir,\n",
    "    raw_dir=raw_dir,\n",
    "    overwrite=False,\n",
    "    cleanup=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup PET directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general.basic.helper_funcs as hf\n",
    "import general.basic.str_methods as strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading /mnt/coredata/processing/leads/metadata/log/raw_pet_scans_2024-03-27-22-54-15.csv\n",
      "raw_scans: (2065, 12)\n"
     ]
    }
   ],
   "source": [
    "raw_scansf = glob_sort_mtime(op.join(PATHS[\"metadata\"], \"log\", \"raw_pet_scans_*.csv\"))[\n",
    "    0\n",
    "]\n",
    "raw_scans = pd.read_csv(raw_scansf)\n",
    "\n",
    "# Remove rows with missing data\n",
    "raw_scans = raw_scans.dropna().reset_index(drop=True)\n",
    "raw_scans = raw_scans.drop(935).reset_index(drop=True)\n",
    "\n",
    "# Fix raw PET filepaths\n",
    "replace_vals = {\n",
    "    \"raw/fbb\": \"raw\",\n",
    "    \"raw/fdg\": \"raw\",\n",
    "    \"raw/ftp\": \"raw\",\n",
    "}\n",
    "raw_scans[\"raw_petf\"] = raw_scans[\"raw_petf\"].apply(\n",
    "    lambda x: strm.str_replace(x, replace_vals)\n",
    ")\n",
    "\n",
    "print(f\"reading {raw_scansf}\")\n",
    "print(f\"raw_scans: {raw_scans.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix raw PET filepaths\n",
    "replace_vals = {\n",
    "    \"raw/fbb\": \"raw\",\n",
    "    \"raw/fdg\": \"raw\",\n",
    "    \"raw/ftp\": \"raw\",\n",
    "}\n",
    "raw_scans[\"raw_petf\"] = raw_scans[\"raw_petf\"].apply(\n",
    "    lambda x: strm.str_replace(x, replace_vals)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all expected files and directories exist\n",
    "raw_scans[\"raw_petf_exists\"] = raw_scans[\"raw_petf\"].apply(lambda x: op.isfile(x))\n",
    "raw_scans[\"mri_dir_exists\"] = raw_scans[\"mri_dir\"].apply(lambda x: op.isdir(x))\n",
    "raw_scans[\"proc_pet_dir_exists\"] = raw_scans[\"proc_pet_dir\"].apply(\n",
    "    lambda x: op.isdir(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scans[\"scan_tag\"] = raw_scans.apply(\n",
    "    lambda x: \"_\".join([x[\"subj\"], x[\"scan_type\"], x[\"scan_date\"]]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>dx</th>\n",
       "      <th>scan_date</th>\n",
       "      <th>scan_type</th>\n",
       "      <th>visit</th>\n",
       "      <th>raw_petf</th>\n",
       "      <th>mri_date</th>\n",
       "      <th>days_to_mri</th>\n",
       "      <th>fs_dir</th>\n",
       "      <th>mri_dir</th>\n",
       "      <th>proc_pet_dir</th>\n",
       "      <th>need_to_process</th>\n",
       "      <th>raw_petf_exists</th>\n",
       "      <th>mri_dir_exists</th>\n",
       "      <th>proc_pet_dir_exists</th>\n",
       "      <th>scan_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>CN</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>FBB</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>LDS0070120_FBB_2019-06-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>CN</td>\n",
       "      <td>2021-07-14</td>\n",
       "      <td>FDG</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>LDS0070120_FDG_2021-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDS0070120</td>\n",
       "      <td>CN</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>FTP</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>LDS0070120_FTP_2019-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>FBB</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>LDS0070166_FBB_2019-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDS0070166</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/LDS007...</td>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>0</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>LDS0070166_FBB_2020-09-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subj    dx   scan_date scan_type  visit  \\\n",
       "0  LDS0070120    CN  2019-06-19       FBB      1   \n",
       "1  LDS0070120    CN  2021-07-14       FDG      1   \n",
       "2  LDS0070120    CN  2019-06-20       FTP      1   \n",
       "3  LDS0070166  EOAD  2019-08-22       FBB      1   \n",
       "4  LDS0070166  EOAD  2020-09-16       FBB      2   \n",
       "\n",
       "                                            raw_petf    mri_date  days_to_mri  \\\n",
       "0  /mnt/coredata/processing/leads/data/raw/LDS007...  2019-06-20            1   \n",
       "1  /mnt/coredata/processing/leads/data/raw/LDS007...  2021-07-13            1   \n",
       "2  /mnt/coredata/processing/leads/data/raw/LDS007...  2019-06-20            0   \n",
       "3  /mnt/coredata/processing/leads/data/raw/LDS007...  2019-08-22            0   \n",
       "4  /mnt/coredata/processing/leads/data/raw/LDS007...  2020-09-16            0   \n",
       "\n",
       "                                              fs_dir  \\\n",
       "0  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "1  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "2  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "3  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "4  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "\n",
       "                                             mri_dir  \\\n",
       "0  /mnt/coredata/processing/leads/data/processed/...   \n",
       "1  /mnt/coredata/processing/leads/data/processed/...   \n",
       "2  /mnt/coredata/processing/leads/data/processed/...   \n",
       "3  /mnt/coredata/processing/leads/data/processed/...   \n",
       "4  /mnt/coredata/processing/leads/data/processed/...   \n",
       "\n",
       "                                        proc_pet_dir  need_to_process  \\\n",
       "0  /mnt/coredata/processing/leads/data/processed/...             True   \n",
       "1  /mnt/coredata/processing/leads/data/processed/...             True   \n",
       "2  /mnt/coredata/processing/leads/data/processed/...             True   \n",
       "3  /mnt/coredata/processing/leads/data/processed/...             True   \n",
       "4  /mnt/coredata/processing/leads/data/processed/...             True   \n",
       "\n",
       "   raw_petf_exists  mri_dir_exists  proc_pet_dir_exists  \\\n",
       "0             True            True                False   \n",
       "1             True            True                False   \n",
       "2             True            True                False   \n",
       "3             True            True                False   \n",
       "4             True            True                False   \n",
       "\n",
       "                    scan_tag  \n",
       "0  LDS0070120_FBB_2019-06-19  \n",
       "1  LDS0070120_FDG_2021-07-14  \n",
       "2  LDS0070120_FTP_2019-06-20  \n",
       "3  LDS0070166_FBB_2019-08-22  \n",
       "4  LDS0070166_FBB_2020-09-16  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2064, 16), (2064, 16))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.shape, raw_scans.query(\"(need_to_process==True)\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "\n",
    "def setup_pet_proc_dirs(raw_scans=None, overwrite=False, verbose=True):\n",
    "    \"\"\"Create processed PET directories and link to associated MRIs.\n",
    "\n",
    "    For each scan that needs to be processed, there must already be:\n",
    "    1. A raw PET file in .nii format\n",
    "    2. A processed MRI directory that will be linked to\n",
    "\n",
    "    This function then does the following for each scan:\n",
    "    1. Creates new processed PET directory\n",
    "    2. Copies raw PET file to processed PET directory and renames it\n",
    "    3. Creates symbolic link from processed PET directory to the\n",
    "       associated MRI directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_scans : DataFrame\n",
    "        A pandas DataFrame with columns 'raw_petf', 'scan_tag',\n",
    "        'mri_dir', and 'proc_pet_dir', which hold paths to raw PET .nii\n",
    "        files, scan tags (\"<subj>_<tracer>_<scan_date>\"), processed MRI\n",
    "        directories that will be used to process each PET scan, and\n",
    "        target directories for processed PET data that will be created\n",
    "        by this function, respectively.\n",
    "    overwrite : bool, optional\n",
    "        If True, overwrite existing processed PET directories if they\n",
    "        exist. If False, skip scans with existing processed PET\n",
    "        directories.\n",
    "    verbose : bool, optional\n",
    "        If True, output status messages during execution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Print the welcome message\n",
    "    if verbose:\n",
    "        title = f\"\\nCreating processed PET directories\"\n",
    "        print(title, \"-\" * len(title), sep=\"\\n\")\n",
    "\n",
    "    # Load the most recently saved raw_scans spreadsheet if not provided\n",
    "    if raw_scans is None:\n",
    "        raw_scansf = glob_sort_mtime(\n",
    "            op.join(PATHS[\"metadata\"], \"log\", \"raw_pet_scans_*.csv\")\n",
    "        )[0]\n",
    "        if verbose:\n",
    "            print(f\"  Reading {raw_scansf}\")\n",
    "        raw_scans = pd.read_csv(raw_scansf)\n",
    "\n",
    "    # Filter scans that need to be processed\n",
    "    raw_scans = raw_scans.query(\"(need_to_process==True)\").reset_index(drop=True)\n",
    "    if verbose:\n",
    "        print(f\"  {raw_scans.shape[0]} scans to process\")\n",
    "\n",
    "    # Loop over each scan and do directory setup\n",
    "    for idx, scan in raw_scans.iterrows():\n",
    "        # Make sure the raw PET file exists\n",
    "        if not op.isfile(scan[\"raw_petf\"]):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"  Skipping {scan['scan_tag']} due to missing raw PET file: {scan['raw_petf']}\"\n",
    "                )\n",
    "            continue\n",
    "        elif not scan[\"raw_petf\"].endswith(\".nii\"):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"  Skipping {scan['scan_tag']} as raw PET file does not end in .nii: {scan['raw_petf']}\"\n",
    "                )\n",
    "            continue\n",
    "        # Make sure the MRI directory exists\n",
    "        if not op.isdir(scan[\"mri_dir\"]):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"  Skipping {scan['scan_tag']} due to missing MRI directory: {scan['mri_dir']}\"\n",
    "                )\n",
    "            continue\n",
    "        # Remove existing processed PET directories if overwrite is True\n",
    "        if op.isdir(scan[\"proc_pet_dir\"]):\n",
    "            if overwrite:\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"  Removing existing directory and its contents: {scan['proc_pet_dir']}\"\n",
    "                    )\n",
    "                shutil.rmtree(scan[\"proc_pet_dir\"])\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"  Skipping {scan['scan_tag']} due to existing processed PET directory: {scan['proc_pet_dir']}\"\n",
    "                    )\n",
    "                continue\n",
    "\n",
    "        # Create the processed PET directory\n",
    "        os.makedirs(scan[\"proc_pet_dir\"])\n",
    "\n",
    "        # Copy the raw PET file to the processed PET directory\n",
    "        infile = scan[\"raw_petf\"]\n",
    "        outfile = op.join(scan[\"proc_pet_dir\"], f\"{scan['scan_tag']}.nii\")\n",
    "        shutil.copy(infile, outfile)\n",
    "\n",
    "        # Create a symlink to the processed MRI directory\n",
    "        link_src = scan[\"mri_dir\"]\n",
    "        link_dst = op.join(scan[\"proc_pet_dir\"], \"mri\")\n",
    "        os.symlink(link_src, link_dst)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>raw_petf_exists</th>\n",
       "      <th>mri_dir_exists</th>\n",
       "      <th>proc_pet_dir_exists</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scan_type</th>\n",
       "      <th>visit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">FBB</th>\n",
       "      <th>1</th>\n",
       "      <td>614/614 (100.0%)</td>\n",
       "      <td>614/614 (100.0%)</td>\n",
       "      <td>0/614 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238/238 (100.0%)</td>\n",
       "      <td>238/238 (100.0%)</td>\n",
       "      <td>0/238 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94/94 (100.0%)</td>\n",
       "      <td>94/94 (100.0%)</td>\n",
       "      <td>0/94 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26/26 (100.0%)</td>\n",
       "      <td>26/26 (100.0%)</td>\n",
       "      <td>0/26 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FDG</th>\n",
       "      <th>1</th>\n",
       "      <td>156/156 (100.0%)</td>\n",
       "      <td>156/156 (100.0%)</td>\n",
       "      <td>0/156 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">FTP</th>\n",
       "      <th>1</th>\n",
       "      <td>604/604 (100.0%)</td>\n",
       "      <td>604/604 (100.0%)</td>\n",
       "      <td>0/604 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>219/219 (100.0%)</td>\n",
       "      <td>219/219 (100.0%)</td>\n",
       "      <td>0/219 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84/84 (100.0%)</td>\n",
       "      <td>84/84 (100.0%)</td>\n",
       "      <td>0/84 (0.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29/29 (100.0%)</td>\n",
       "      <td>29/29 (100.0%)</td>\n",
       "      <td>0/29 (0.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  raw_petf_exists    mri_dir_exists proc_pet_dir_exists\n",
       "scan_type visit                                                        \n",
       "FBB       1      614/614 (100.0%)  614/614 (100.0%)        0/614 (0.0%)\n",
       "          2      238/238 (100.0%)  238/238 (100.0%)        0/238 (0.0%)\n",
       "          3        94/94 (100.0%)    94/94 (100.0%)         0/94 (0.0%)\n",
       "          4        26/26 (100.0%)    26/26 (100.0%)         0/26 (0.0%)\n",
       "FDG       1      156/156 (100.0%)  156/156 (100.0%)        0/156 (0.0%)\n",
       "FTP       1      604/604 (100.0%)  604/604 (100.0%)        0/604 (0.0%)\n",
       "          2      219/219 (100.0%)  219/219 (100.0%)        0/219 (0.0%)\n",
       "          3        84/84 (100.0%)    84/84 (100.0%)         0/84 (0.0%)\n",
       "          4        29/29 (100.0%)    29/29 (100.0%)         0/29 (0.0%)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.groupby([\"scan_type\", \"visit\"]).agg(\n",
    "    {\n",
    "        \"raw_petf_exists\": hf.count_pct,\n",
    "        \"mri_dir_exists\": hf.count_pct,\n",
    "        \"proc_pet_dir_exists\": hf.count_pct,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "need_to_process  scan_type  visit\n",
       "True             FBB        1        614\n",
       "                            2        238\n",
       "                            3         94\n",
       "                            4         26\n",
       "                 FDG        1        156\n",
       "                 FTP        1        604\n",
       "                            2        219\n",
       "                            3         84\n",
       "                            4         29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.groupby([\"need_to_process\", \"scan_type\", \"visit\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/coredata/processing/leads/data/processed/LDS0070120/FBB_2019-06-19'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.iloc[0][\"proc_pet_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: LDS0730083 FTP scan on 2021-01-19 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0730083/Timepoint*/FTP_2021-01-19\n",
      "WARNING: LDS0730150 FTP scan on 2021-11-10 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0730150/Timepoint*/FTP_2021-11-10\n",
      "WARNING: LDS0730150 FBB scan on 2021-12-09 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0730150/Timepoint*/FBB_2021-12-09\n",
      "WARNING: LDS0370015 FBB scan on 2022-05-10 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0370015/Timepoint*/FBB_2022-05-10\n",
      "WARNING: LDS9410395 FDG scan on 2022-08-03 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS9410395/Timepoint*/FDG_2022-08-03\n",
      "WARNING: LDS0220071 FBB scan on 2022-08-09 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0220071/Timepoint*/FBB_2022-08-09\n",
      "WARNING: LDS0350342 FBB scan on 2022-08-31 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0350342/Timepoint*/FBB_2022-08-31\n",
      "WARNING: LDS0350342 FTP scan on 2022-09-21 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0350342/Timepoint*/FTP_2022-09-21\n",
      "WARNING: LDS0070313 FBB scan on 2022-10-12 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0070313/Timepoint*/FBB_2022-10-12\n",
      "WARNING: LDS0070313 FTP scan on 2022-10-13 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0070313/Timepoint*/FTP_2022-10-13\n",
      "WARNING: LDS0220071 FTP scan on 2022-10-18 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0220071/Timepoint*/FTP_2022-10-18\n",
      "WARNING: LDS0370038 FBB scan on 2023-01-05 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0370038/Timepoint*/FBB_2023-01-05\n",
      "WARNING: LDS0820514 FTP scan on 2023-03-06 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0820514/Timepoint*/FTP_2023-03-06\n",
      "WARNING: LDS0350171 FBB scan on 2023-11-01 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0350171/Timepoint*/FBB_2023-11-01\n",
      "WARNING: LDS0350171 FTP scan on 2023-11-30 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0350171/Timepoint*/FTP_2023-11-30\n",
      "WARNING: LDS9410287 FTP scan on 2024-01-31 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS9410287/Timepoint*/FTP_2024-01-31\n",
      "WARNING: LDS3600455 FTP scan on 2024-02-02 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS3600455/Timepoint*/FTP_2024-02-02\n",
      "WARNING: LDS3600455 FBB scan on 2024-02-06 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS3600455/Timepoint*/FBB_2024-02-06\n",
      "WARNING: LDS0370316 FBB scan on 2024-02-15 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0370316/Timepoint*/FBB_2024-02-15\n",
      "WARNING: LDS9410572 FBB scan on 2024-02-20 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS9410572/Timepoint*/FBB_2024-02-20\n",
      "WARNING: LDS1770264 FBB scan on 2024-02-22 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS1770264/Timepoint*/FBB_2024-02-22\n",
      "WARNING: LDS0220273 FTP scan on 2024-02-22 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0220273/Timepoint*/FTP_2024-02-22\n",
      "WARNING: LDS3600571 FTP scan on 2024-03-01 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS3600571/Timepoint*/FTP_2024-03-01\n",
      "WARNING: LDS9410459 FBB scan on 2024-03-05 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS9410459/Timepoint*/FBB_2024-03-05\n",
      "WARNING: LDS3600421 FTP scan on 2024-03-05 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS3600421/Timepoint*/FTP_2024-03-05\n",
      "WARNING: LDS3600542 FBB scan on 2024-03-05 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS3600542/Timepoint*/FBB_2024-03-05\n",
      "WARNING: LDS3600571 FBB scan on 2024-03-07 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS3600571/Timepoint*/FBB_2024-03-07\n",
      "WARNING: LDS0370449 FTP scan on 2024-03-11 is missing a processed PET dir in /mnt/coredata/Projects/LEADS/data_f7p1/processed/LDS0370449/Timepoint*/FTP_2024-03-11\n"
     ]
    }
   ],
   "source": [
    "file_renaming_map = {\n",
    "    \"FBB\": {\n",
    "        op.join(pet_dir_old, \"compwm_ref_mask.nii\"): op.join(mri_dir, \"\"),\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "    },\n",
    "    \"FTP\": {\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "        op.join(pet_dir_old, \"\"): op.join(mri_dir, \"\"),\n",
    "    },\n",
    "    \"MRI-T1\": {},\n",
    "}\n",
    "\n",
    "# copy mask files from PET to MRI proc dirs\n",
    "for idx, scan in (\n",
    "    raw_scans.query(\"(scan_type==['FBB','FTP'])\").sort_values(\"scan_date\").iterrows()\n",
    "):\n",
    "    try:\n",
    "        globstr = f\"/mnt/coredata/Projects/LEADS/data_f7p1/processed/{scan['subj']}/Timepoint*/{scan['scan_type']}_{scan['scan_date']}\"\n",
    "        pet_dir_old = glob(globstr)[0]\n",
    "    except IndexError:\n",
    "        print(\n",
    "            f\"WARNING: {scan['subj']} {scan['scan_type']} scan on {scan['scan_date']} is missing a processed PET dir in {globstr}\"\n",
    "        )\n",
    "        continue\n",
    "    try:\n",
    "        globstr = f\"/mnt/coredata/Projects/LEADS/data_f7p1/processed/{scan['subj']}/Timepoint*/MRI_T1_{scan['mri_date']}\"\n",
    "        mri_dir_old = glob(globstr)[0]\n",
    "    except IndexError:\n",
    "        print(\n",
    "            f\"WARNING: {scan['subj']} MRI scan on {scan['scan_date']} is missing a processed MRI dir in {globstr}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # copy mask files from PET to MRI proc dirs\n",
    "    mri_dir = scan[\"mri_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>dx</th>\n",
       "      <th>scan_date</th>\n",
       "      <th>scan_type</th>\n",
       "      <th>visit</th>\n",
       "      <th>raw_petf</th>\n",
       "      <th>mri_date</th>\n",
       "      <th>days_to_mri</th>\n",
       "      <th>fs_dir</th>\n",
       "      <th>mri_dir</th>\n",
       "      <th>proc_pet_dir</th>\n",
       "      <th>need_to_process</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>FBB</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/fbb/LD...</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>0</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>FBB</td>\n",
       "      <td>2</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/fbb/LD...</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>0</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2020-10-28</td>\n",
       "      <td>FBB</td>\n",
       "      <td>3</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/fbb/LD...</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>FBB</td>\n",
       "      <td>4</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/fbb/LD...</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>13</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>FTP</td>\n",
       "      <td>1</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/ftp/LD...</td>\n",
       "      <td>2018-08-15</td>\n",
       "      <td>12</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>FTP</td>\n",
       "      <td>2</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/ftp/LD...</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>14</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>FTP</td>\n",
       "      <td>3</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/ftp/LD...</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>0</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>LDS0370008</td>\n",
       "      <td>EOAD</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>FTP</td>\n",
       "      <td>4</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/raw/ftp/LD...</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>36</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/freesurfer...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>/mnt/coredata/processing/leads/data/processed/...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subj    dx   scan_date scan_type  visit  \\\n",
       "680  LDS0370008  EOAD  2018-08-15       FBB      1   \n",
       "681  LDS0370008  EOAD  2019-09-19       FBB      2   \n",
       "682  LDS0370008  EOAD  2020-10-28       FBB      3   \n",
       "683  LDS0370008  EOAD  2021-11-16       FBB      4   \n",
       "684  LDS0370008  EOAD  2018-08-27       FTP      1   \n",
       "685  LDS0370008  EOAD  2019-10-03       FTP      2   \n",
       "686  LDS0370008  EOAD  2020-10-29       FTP      3   \n",
       "687  LDS0370008  EOAD  2021-12-09       FTP      4   \n",
       "\n",
       "                                              raw_petf    mri_date  \\\n",
       "680  /mnt/coredata/processing/leads/data/raw/fbb/LD...  2018-08-15   \n",
       "681  /mnt/coredata/processing/leads/data/raw/fbb/LD...  2019-09-19   \n",
       "682  /mnt/coredata/processing/leads/data/raw/fbb/LD...  2020-10-29   \n",
       "683  /mnt/coredata/processing/leads/data/raw/fbb/LD...  2021-11-03   \n",
       "684  /mnt/coredata/processing/leads/data/raw/ftp/LD...  2018-08-15   \n",
       "685  /mnt/coredata/processing/leads/data/raw/ftp/LD...  2019-09-19   \n",
       "686  /mnt/coredata/processing/leads/data/raw/ftp/LD...  2020-10-29   \n",
       "687  /mnt/coredata/processing/leads/data/raw/ftp/LD...  2021-11-03   \n",
       "\n",
       "     days_to_mri                                             fs_dir  \\\n",
       "680            0  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "681            0  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "682            1  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "683           13  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "684           12  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "685           14  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "686            0  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "687           36  /mnt/coredata/processing/leads/data/freesurfer...   \n",
       "\n",
       "                                               mri_dir  \\\n",
       "680  /mnt/coredata/processing/leads/data/processed/...   \n",
       "681  /mnt/coredata/processing/leads/data/processed/...   \n",
       "682  /mnt/coredata/processing/leads/data/processed/...   \n",
       "683  /mnt/coredata/processing/leads/data/processed/...   \n",
       "684  /mnt/coredata/processing/leads/data/processed/...   \n",
       "685  /mnt/coredata/processing/leads/data/processed/...   \n",
       "686  /mnt/coredata/processing/leads/data/processed/...   \n",
       "687  /mnt/coredata/processing/leads/data/processed/...   \n",
       "\n",
       "                                          proc_pet_dir  need_to_process  \n",
       "680  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "681  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "682  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "683  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "684  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "685  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "686  /mnt/coredata/processing/leads/data/processed/...             True  \n",
       "687  /mnt/coredata/processing/leads/data/processed/...             True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.query(\"(subj=='LDS0370008')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EOAD       396\n",
       "EOnonAD    122\n",
       "CN          99\n",
       "NaN          1\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scans.drop_duplicates(\"subj\")[\"dx\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_scans: (2067, 12)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
